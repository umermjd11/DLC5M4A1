{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: auto; text-align: center; width: 100%; table-layout: fixed;\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a href=\"mailto:umermjd11@gmail.com\" target=\"_blank\">\n",
    "                <img src=\"https://img.shields.io/badge/Email-umermjd11@gmail.com-informational?style=for-the-badge&logo=gmail&logoColor=white&color=FF5722\" alt=\"Email Shield\">\n",
    "            </a>\n",
    "        </td>\n",
    "        <td>\n",
    "            <a href=\"https://umermjd11.github.io/\" target=\"_blank\">\n",
    "                <img src=\"https://img.shields.io/badge/Website-umermjd11.github.com-2F2E41?style=for-the-badge&logo=google-chrome&logoColor=white\" alt=\"Website Shield\">\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a href=\"https://github.com/umermjd11\" target=\"_blank\">\n",
    "                <img src=\"https://img.shields.io/badge/GitHub-umermjd11-181717?style=for-the-badge&logo=github&logoColor=white\" alt=\"GitHub Shield\">\n",
    "            </a>\n",
    "        </td>\n",
    "        <td>\n",
    "            <a href=\"https://kaggle.com/umermjd11\" target=\"_blank\">\n",
    "                <img src=\"https://img.shields.io/badge/Kaggle-umermjd11-20BEFF?style=for-the-badge&logo=kaggle&logoColor=white\" alt=\"Kaggle Shield\">\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a href=\"https://linkedin.com/in/umermjd11\" target=\"_blank\">\n",
    "                <img src=\"https://img.shields.io/badge/LinkedIn-umermjd11-blue?style=for-the-badge&logo=linkedin&logoColor=white\" alt=\"LinkedIn Shield\">\n",
    "            </a>\n",
    "        </td>\n",
    "        <td>\n",
    "            <a href=\"https://scholar.google.com/citations?user=LrsLEJgAAAAJ&hl=en\" target=\"_blank\">\n",
    "                <img src=\"https://img.shields.io/badge/Google%20Scholar-LrsLEJgAAAAJ-4285F4?style=for-the-badge&logo=google-scholar&logoColor=white\" alt=\"Google Scholar Shield\">\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a href=\"https://orcid.org/0000-0002-5908-3889\" target=\"_blank\">\n",
    "                <img src=\"https://img.shields.io/badge/ORCID-0000_0002_5908_3889-A6CE39?style=for-the-badge&logo=orcid&logoColor=white\" alt=\"ORCID Shield\">\n",
    "            </a>\n",
    "        </td>\n",
    "        <td>\n",
    "            <a href=\"https://www.researchgate.net/profile/Umer-Majeed\" target=\"_blank\">\n",
    "                <img src=\"https://img.shields.io/badge/ResearchGate-Umer_Majeed-00CCBB?style=for-the-badge&logo=researchgate&logoColor=white\" alt=\"ResearchGate Shield\">\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"2\">\n",
    "            <a href=\"https://twitter.com/umermjd11\" target=\"_blank\">\n",
    "                <img src=\"https://img.shields.io/badge/Twitter-umermjd11-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white\" alt=\"Twitter Shield\">\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<h1 style=\"background: #FFC07F; border: 0; color: #2F2E41; \n",
    "    box-shadow: 4px 4px 8px rgba(0, 0, 0, 0.3); \n",
    "    padding: 10px; border-radius: 10px; margin: 15px 0;\">\n",
    "    <center style=\"color: #2F2E41;\">1. Overview</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1-1\"></a>\n",
    "\n",
    "## 1.1. Transformer Network\n",
    "\n",
    "\n",
    "In this analysis, we explore the Transformer architecture, a powerful neural network model that leverages parallel processing to accelerate training while capturing sequential relationships in data. Unlike sequential models such as RNNs, GRUs, and LSTMs, Transformers utilize self-attention mechanisms, enabling more efficient and scalable learning of complex patterns in sequences.\n",
    "\n",
    "<a id=\"1-2\"></a>\n",
    "\n",
    "## 1.2. Objectives\n",
    "\n",
    "Through this study, we aim to:\n",
    "\n",
    "- Develop positional encodings to represent the order of elements in sequences.\n",
    "- Compute scaled dot-product self-attention for efficient contextualization of embeddings.\n",
    "- Implement masked multi-head attention to manage sequence dependencies.\n",
    "- Construct and train a Transformer model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "<h1 style=\"background: #FFC07F; border: 0; color: #2F2E41; \n",
    "    box-shadow: 4px 4px 8px rgba(0, 0, 0, 0.3); \n",
    "    padding: 10px; border-radius: 10px; margin: 15px 0;\">\n",
    "    <center style=\"color: #2F2E41;\">2. Imports</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 20:47:54.720018: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-23 20:47:56.314587: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-23 20:47:56.835657: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732394877.546540    1139 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732394877.743706    1139 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-23 20:47:59.492407: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/user/dl/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization\n",
    "from transformers import DistilBertTokenizerFast #, TFDistilBertModel\n",
    "from transformers import TFDistilBertForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "<h1 style=\"background: #FFC07F; border: 0; color: #2F2E41; \n",
    "    box-shadow: 4px 4px 8px rgba(0, 0, 0, 0.3); \n",
    "    padding: 10px; border-radius: 10px; margin: 15px 0;\">\n",
    "    <center style=\"color: #2F2E41;\">3. Positional Encoding</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike RNNs, which process inputs sequentially and inherently capture order information, Transformers process the entire input sequence in parallel. While this parallelism significantly reduces training time, it also removes the natural sense of order.\n",
    "\n",
    "<a id=\"3-1\"></a>\n",
    "\n",
    "## 3.1. Objective of Positional Encoding\n",
    "\n",
    "To address the limitation of the Transformer architecture's lack of inherent sequential processing, positional encodings are introduced to explicitly provide information about the order of input data. These encodings ensure that sequence order is preserved while maintaining the parallelism advantage of Transformers. The inclusion of positional encodings ensures that:\n",
    "\n",
    "1. The Transformer can understand the relative and absolute positions of tokens in a sequence.\n",
    "2. The encoded sequence retains both semantic information (from embeddings) and positional information (from encodings).\n",
    "3. The model captures dependencies between words based on their positions, which is critical for tasks like language modeling and translation.\n",
    "\n",
    "<a id=\"3-2\"></a>\n",
    "\n",
    "## 3.2. Process of Positional Encoding\n",
    "\n",
    "The process involves integrating mathematical patterns of sine and cosine functions with word embeddings to encode positional information. Here's how it works:\n",
    "\n",
    "### 3.2.1. Mathematical Definition\n",
    "\n",
    "Positional encodings are computed using two equations:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$d$** is the dimensionality of the encoding and the word embedding.\n",
    "- **$pos$** is the position index of the token in the sequence.\n",
    "- **$i$** is an integer index that runs over the half of dimensions of the encoding vector. $i$ represents half the dimension index of the encoding.\n",
    "\n",
    "This pattern ensures that even and odd dimensions have distinct positional information, with sine and cosine functions respectively.\n",
    "\n",
    "### 3.2.2. Understanding $pos$, $d$, and $i$\n",
    "\n",
    "- **$pos$**: In positional encoding, **$pos$** refers to the position of the token in the sequence. **$pos$** is crucial for the model to differentiate between tokens based on their order. **$pos$** starts at 0. For example, in a sequence `[\"I\", \"am\", \"learning\", \"Transformers\"]`, the positions would be:\n",
    "    - \"I\" → **$pos = 0$**\n",
    "    - \"am\" → **$pos = 1$**\n",
    "    - \"learning\" → **$pos = 2$**\n",
    "    - \"Transformers\" → **$pos = 3$**\n",
    "    \n",
    "   **Why Start from 0?** This is because in most programming languages, indexing typically starts from zero, which is also the case for Transformer models. This zero-indexed convention aligns with standard practices in machine learning.\n",
    "\n",
    "- **$d$**: **$d$** refers to the total dimensionality of the positional encoding vector. This is the size of the vector that will be added to the token embeddings. For instance, if **$d = 6$**, each positional encoding vector would have 6 dimensions. It has same dimensions as the embedding vector.\n",
    "\n",
    "- **$i$**: **$i$** controls how the positional information is spread across the dimensions. The term \"half the dimension index\" means that $i$ iterates over half of the total dimensions ($d$) of encoding. **$i$** determines how the position will be encoded within each dimension of the positional encoding vector. Each value of **$i$** corresponds to two dimensions: one for sine and one for cosine. The idea of \"half the dimension index\" comes from the fact that the encoding for each dimension alternates between sine (for even indices) and cosine (for odd indices).\n",
    "\n",
    "#### Example:\n",
    "\n",
    "###### Case 1: **When $d = 6$ (Even)**\n",
    "\n",
    "For **$d = 6$**, the indices of the dimensions are $0, 1, 2, 3, 4, 5$, **$i$** will iterate from **0 to 2** because:\n",
    "- There are 6 dimensions in total, with half for sine and half for cosine functions. The sine function is applied to even indices ($0, 2, 4$), and the cosine function is applied to odd indices ($1, 3, 5$).\n",
    "- For **$i = 0$**, sine and cosine will be computed for the first two dimensions: **$PE_{(pos, 0)}$** and **$PE_{(pos, 1)}$**.\n",
    "- For **$i = 1$**, sine and cosine will be computed for the next two dimensions: **$PE_{(pos, 2)}$** and **$PE_{(pos, 3)}$**.\n",
    "- For **$i = 2$**, sine and cosine will be computed for the next two dimensions: **$PE_{(pos, 4)}$** and **$PE_{(pos, 5)}$**.\n",
    "\n",
    "###### Case 2: **When $d = 7$ (Odd)**\n",
    "\n",
    "For **$d = 7$**, the indices of the dimensions are $0, 1, 2, 3, 4, 5, 6$, **$i$** will iterate from **0 to 3** because:\n",
    "- There are 7 dimensions in total, with half for sine and half for cosine functions. The sine function is applied to even indices ($0, 2, 4, 6$), and the cosine function is applied to odd indices ($1, 3, 5$).\n",
    "- For **$i = 0$**, sine and cosine will be computed for the first two dimensions: **$PE_{(pos, 0)}$** and **$PE_{(pos, 1)}$**.\n",
    "- For **$i = 1$**, sine and cosine will be computed for the next two dimensions: **$PE_{(pos, 2)}$** and **$PE_{(pos, 3)}$**.\n",
    "- For **$i = 2$**, sine and cosine will be computed for the next two dimensions: **$PE_{(pos, 4)}$** and **$PE_{(pos, 5)}$**.\n",
    "- For **$i = 3$** handles the remaining dimension **$PE_{(pos, 6)}$** (this will typically be a **sine** or cosine depending on the specific design, often treated separately in implementation).\n",
    "\n",
    "##### General Rule:\n",
    "- If the total dimensionality **$d$** is even (say, **$d = 6$**), the values of **$i$** will range from 0 to **$d/2 - 1$**.\n",
    "- **When $d$ is odd**, **$i$** iterates from 0 to **$\\lceil d/2 \\rceil - 1$** (the ceiling of half the dimensions).\n",
    "\n",
    "### 3.2.3. Key Features of Sine and Cosine Functions\n",
    "\n",
    "- **Periodicity**: These functions introduce periodic patterns, which help the model generalize over varying sequence lengths and positions.\n",
    "- **Range**: The values are bounded between **$-1$** and **$1$**, ensuring that adding positional encodings to word embeddings does not disrupt semantic information.\n",
    "- **Unique Representation**: The use of sine for even indices and cosine for odd indices ensures unique positional information for each dimension. The sine and cosine functions with different frequency scaling ($10000^{\\frac{2i}{d}}$) create unique encoding for each position **$pos$**.\n",
    "\n",
    "### 3.2.4. Encoding as Input\n",
    "\n",
    "For a given sequence of tokens, the positional encodings are computed and added to the word embeddings. This combined representation is fed into the Transformer, enriching the input with both semantic and positional information. This enables the model to:\n",
    "- Recognize the position of each token.\n",
    "- Compute dependencies between tokens based on their relative positions.\n",
    "\n",
    "### 3.2.5. Visualization of Encodings\n",
    "\n",
    "The positional encodings for a sequence can be represented as:\n",
    "\n",
    "|      k         | <code>       0      </code>|<code>       1      </code>|<code>       2      </code>|<code>       3      </code>| <code> ... </code> |<code>      d - 2     </code>|<code>      d - 1     </code>| \n",
    "| ---------------- | :------: | ----------------- | ----------------- | ----------------- | ----- | ----------------- | ----------------- |\n",
    "| encoding(0) = |[$sin(\\theta(0, 0, d))$| $cos(\\theta(0, 0, d))$| $sin(\\theta(0, 1, d))$| $cos(\\theta(0, 1, d))$|... |$sin(\\theta(0, d//2, d))$| $cos(\\theta(0, d//2, d))$]|\n",
    "| encoding(1) = | [$sin(\\theta(1, 0, d))$| $cos(\\theta(1, 0, d))$| $sin(\\theta(1, 1, d))$| $cos(\\theta(1, 1, d))$|... |$sin(\\theta(1, d//2, d))$| $cos(\\theta(1, d//2, d))$]|\n",
    "...\n",
    "| encoding(pos) = | [$sin(\\theta(pos, 0, d))$| $cos(\\theta(pos, 0, d))$| $sin(\\theta(pos, 1, d))$| $cos(\\theta(pos, 1, d))$|... |$sin(\\theta(pos, d//2, d))$| $cos(\\theta(pos, d//2, d))]$|\n",
    "\n",
    "### 3.2.6. Inner Term: Angle Calculation\n",
    "\n",
    "Notice that even though the sine and cosine positional encoding equations take in different arguments (`2i` versus `2i+1`, or even versus odd numbers) the inner terms for both equations are the same: $$\\theta(pos, i, d) = \\frac{pos}{10000^{\\frac{2i}{d}}} \\tag{3}$$\n",
    "\n",
    "Consider the inner term as you calculate the positional encoding for a word in a sequence.<br> \n",
    "$PE_{(pos, 0)}= sin\\left(\\frac{pos}{{10000}^{\\frac{0}{d}}}\\right)$, since solving `2i = 0` gives `i = 0` <br>\n",
    "$PE_{(pos, 1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{0}{d}}}\\right)$, since solving `2i + 1 = 1` gives `i = 0`\n",
    "\n",
    "\n",
    "The angle is the same for both! The angles for $PE_{(pos, 2)}$ and $PE_{(pos, 3)}$ are the same as well, since for both, `i = 1`. \n",
    "\n",
    "This shared term ensures that the encodings for each token are consistent and represent the relative order of tokens within the sequence.\n",
    "\n",
    "<a id=\"3-3\"></a>\n",
    "\n",
    "## 3.3. Why Sine and Cosine?\n",
    "\n",
    "These periodic functions encode positional information without distorting the semantics of word embeddings. Their values range between **$-1$** and **$1$**, ensuring that when added to word embeddings, the original semantic information is enriched, not overwritten. Furthermore, the use of two distinct functions introduces unique positional features for each dimension, allowing the model to effectively learn positional relationships.\n",
    "\n",
    "### 3.3.1. Key Features of Sine and Cosine Functions\n",
    "\n",
    "- **Periodicity**: These functions introduce periodic patterns, which help the model generalize over varying sequence lengths and positions.\n",
    "- **Range**: The values are bounded between **$-1$** and **$1$**, ensuring that adding positional encodings to word embeddings does not disrupt semantic information.\n",
    "- **Unique Representation**: The use of sine for even indices and cosine for odd indices ensures unique positional information for each dimension.\n",
    "\n",
    "### 3.3.2. Smooth Interpolation\n",
    "\n",
    "The continuous nature of sine and cosine functions allows the model to generalize positional information for sequences of varying lengths.\n",
    "\n",
    "### 3.3.3. Differentiability\n",
    "\n",
    "These functions are fully differentiable, making them compatible with gradient-based optimization.\n",
    "\n",
    "### 3.3.4. Encoding Relative Positions\n",
    "\n",
    "The periodicity inherently encodes both absolute and relative positional relationships, enabling the model to understand word dependencies within a sequence.\n",
    "\n",
    "<a id=\"3-4\"></a>\n",
    "\n",
    "## 3.4. Summary\n",
    "\n",
    "Positional encoding is a critical component of the Transformer architecture, enabling it to process sequences effectively while maintaining the relationships between tokens. By combining mathematical rigor and practical utility, positional encodings allow Transformers to achieve state-of-the-art performance across various natural language processing tasks.\n",
    "\n",
    "<a id=\"3-5\"></a>\n",
    "\n",
    "## 3.5. get_angles() implementation\n",
    "\n",
    "#### **Objective**\n",
    "The function `get_angles()` computes the angular values required for sine and cosine positional encodings used in Transformer architectures. The goal is to generate a matrix where each position (row) and encoding dimension (column) contain the appropriate angular values based on their respective indices. These angular values are later passed to sine and cosine functions to create positional encodings.\n",
    "\n",
    "#### **Function Description**\n",
    "\n",
    "1. **Inputs:**\n",
    "   - `pos`: A column vector representing the position indices of tokens in the sequence, e.g., \\([[0], [1], [2], \\dots, [N-1]]\\).\n",
    "   - `k`: A row vector representing the dimension span indices, e.g., \\([[0, 1, 2, \\dots, d-1]]\\).\n",
    "   - `d`: An integer denoting the total number of encoding dimensions.\n",
    "\n",
    "2. **Outputs:**\n",
    "   - `angles`: A NumPy array of shape \\((N, d)\\), where \\(N\\) is the number of positions, and \\(d\\) is the encoding dimension. Each element corresponds to the angular value calculated using the formula:\n",
    "     $$\n",
    "     \\text{angles}(pos, k) = \\frac{pos}{10000^{\\frac{2i}{d}}}\n",
    "     $$\n",
    "     where \\(i = \\lfloor \\frac{k}{2} \\rfloor\\) to alternate sine and cosine dimensions.\n",
    "\n",
    "\n",
    "#### **Implementation Details**\n",
    "\n",
    "1. **Extracting \\(i\\):**\n",
    "   - \\(i\\) is calculated as `k // 2`. This maps each dimension index (\\(k\\)) to the \"half-dimension index\" (\\(i\\)).\n",
    "   - For example:\n",
    "     - If \\(k = [0, 1, 2, 3, 4, 5]\\), then \\(i = [0, 0, 1, 1, 2, 2]\\).\n",
    "\n",
    "2. **Angle Calculation:**\n",
    "   - For each position \\(pos\\) and dimension \\(k\\), the angle is computed as:\n",
    "     $$\n",
    "     \\frac{pos}{10000^{\\frac{2i}{d}}}\n",
    "     $$\n",
    "   - Here, \\(pos\\) controls the token position, \\(d\\) is the total encoding size, and \\(i\\) ensures that dimensions alternate between sine and cosine patterns.\n",
    "\n",
    "3. **Matrix Generation:**\n",
    "   - Using vectorized operations, the function computes the angle matrix of shape \\((N, d)\\), where \\(N\\) is the number of positions in the input sequence and \\(d\\) is the encoding dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, k, d):\n",
    "    \"\"\"\n",
    "    Get the angles for the positional encoding\n",
    "    \n",
    "    Arguments:\n",
    "        pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
    "        k --   Row vector containing the dimension span [[0, 1, 2, ..., d-1]]\n",
    "        d(integer) -- Encoding size\n",
    "    \n",
    "    Returns:\n",
    "        angles -- (pos, d) numpy array \n",
    "    \"\"\"\n",
    "    # Get i from dimension span k\n",
    "    i = k // 2  # Calculate the \"half-dimension index\" using integer division for alternating sine (k=2*i) and cosine (k=2*i+1) \n",
    "    \n",
    "    # Calculate the angles using pos, i, and d\n",
    "    angles = pos / (np.power(10000, (2 * i) / np.float32(d)))\n",
    "    \n",
    "    return angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.080s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from public_tests import *\n",
    "# Unit Test Class\n",
    "class TestGetAngles(unittest.TestCase):\n",
    "    def test_get_angles(self):\n",
    "        # Validate correctness using provided test utility\n",
    "        get_angles_test(get_angles)\n",
    "    \n",
    "    def test_example_case(self):\n",
    "        # Example case\n",
    "        position = 4\n",
    "        d_model = 8\n",
    "        pos_m = np.arange(position)[:, np.newaxis]  # Shape (4, 1)\n",
    "        dims = np.arange(d_model)[np.newaxis, :]  # Shape (1, 8)\n",
    "        \n",
    "        # Expected output for manual validation\n",
    "        expected_angles = pos_m / (10000 ** (2 * (dims // 2) / np.float32(d_model)))\n",
    "        \n",
    "        # Actual output\n",
    "        actual_angles = get_angles(pos_m, dims, d_model)\n",
    "        \n",
    "        # Assert the arrays are almost equal (floating-point precision)\n",
    "        np.testing.assert_almost_equal(actual_angles, expected_angles, decimal=6)\n",
    "\n",
    "# Running the test cases in Jupyter Notebook\n",
    "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestGetAngles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-6\"></a>\n",
    "\n",
    "## 3.6. `positional_encoding` Implementation\n",
    "The `positional_encoding` function implements the sine and cosine positional encoding mechanism for Transformer models. This encoding incorporates positional information into token embeddings, enabling the model to discern the sequence order of inputs.\n",
    "\n",
    "#### **Objective**\n",
    "To calculate positional encodings for a specified number of positions and embedding dimensions ($d$), as defined by the equations:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "The goal is to precompute a matrix of positional encodings such that even indices use the sine function, and odd indices use the cosine function. This approach ensures that each position and dimension have a unique encoding, enabling the model to capture relative and absolute positional relationships.\n",
    "\n",
    "#### **Guidelines for the implementaton**\n",
    "\n",
    "1. **Initialize Angles**:\n",
    "   - **`pos[:, np.newaxis]`** creates a column vector (shape `(positions, 1)`), making it compatible for broadcasting.\n",
    "   - **`k[np.newaxis, :]`** creates a row vector (shape `(1, d)`), making it compatible for broadcasting.\n",
    "   - Calls the `get_angles` function to compute the angles based on position indices (`pos`) and dimension indices (`i`). \n",
    "   - **`pos` (column vector)** is broadcasted across each column.\n",
    "   - **`10000^(2i/d)`** (row vector) is broadcasted across each row.\n",
    "   - This forms the matrix `angle_rads` of shape `(positions, d)`, where:\n",
    "$$\n",
    "\\text{angles}_{p, k} = \\frac{\\text{pos}_p}{10000^{\\frac{2i}{d}}}\n",
    "$$\n",
    "\n",
    "2. **Apply Sine to Even Indices**:\n",
    "   - For every second index (`0, 2, 4, ...`), the sine function is applied:\n",
    "     ```python\n",
    "     angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "     ```\n",
    "   - This corresponds to the $PE_{(pos, 2i)}$ formula.\n",
    "\n",
    "3. **Apply Cosine to Odd Indices**:\n",
    "   - For every alternate index (`1, 3, 5, ...`), the cosine function is applied:\n",
    "     ```python\n",
    "     angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "     ```\n",
    "   - This corresponds to the $PE_{(pos, 2i+1)}$ formula.\n",
    "\n",
    "4. **Add an Extra Dimension**:\n",
    "   - The positional encoding matrix is reshaped to `(1, positions, d)` by adding a batch dimension for compatibility with neural network models:\n",
    "     ```python\n",
    "     pos_encoding = angle_rads[np.newaxis, ...]\n",
    "     ```\n",
    "\n",
    "5. **Convert to Tensor**:\n",
    "   - The final matrix is cast to a TensorFlow tensor with a data type of `float32`:\n",
    "     ```python\n",
    "     tf.cast(pos_encoding, dtype=tf.float32)\n",
    "     ```\n",
    "\n",
    "#### **Flow of the Implementation**\n",
    "1. Compute angles for positional encodings using `get_angles`.\n",
    "2. Apply sine and cosine functions to even and odd indices, respectively.\n",
    "3. Reshape the encoding to include a batch dimension.\n",
    "4. Return the positional encoding matrix as a TensorFlow tensor.\n",
    "\n",
    "#### **Result**\n",
    "The output is a matrix of shape `(1, positions, d)` containing precomputed positional encodings. This matrix can be added to token embeddings to incorporate positional information while maintaining compatibility with Transformer architectures. \n",
    "\n",
    "This implementation follows the theoretical framework of positional encoding and adheres to the equations defining the sine and cosine functions for even and odd indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        positions (int) -- Maximum number of positions to be encoded \n",
    "        d (int) -- Encoding size \n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding -- (1, position, d_model) A matrix with the positional encodings\n",
    "    \"\"\"\n",
    "    # START CODE HERE\n",
    "    # initialize a matrix angle_rads of all the angles  \n",
    "    angle_rads = get_angles(np.arange(positions)[:, np.newaxis],\n",
    "                            np.arange(d)[ np.newaxis,:],\n",
    "                            d)\n",
    "  \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    # END CODE HERE\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 20:48:15.074079: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST    \n",
    "positional_encoding_test(positional_encoding, get_angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice work calculating the positional encodings! Now we can visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('d')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents a positional encoding - notice how none of the rows are identical! We have created a unique positional encoding for each of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<h1 style=\"background: #FFC07F; border: 0; color: #2F2E41; \n",
    "    box-shadow: 4px 4px 8px rgba(0, 0, 0, 0.3); \n",
    "    padding: 10px; border-radius: 10px; margin: 15px 0;\">\n",
    "    <center style=\"color: #2F2E41;\">4. Masking</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer networks rely heavily on the attention mechanism, which computes the relationships between tokens in a sequence. The attention mechanism uses a softmax function to assign weights to each token, determining its relevance. However, input sequences often vary in length, necessitating adjustments to ensure consistency during computation.\n",
    "\n",
    "When constructing a Transformer network, two types of masks are critical for ensuring the appropriate functionality of the attention mechanism: the **padding mask** and the **look-ahead mask**. These masks ensure that the softmax computation assigns weights correctly, maintaining the integrity of sequence processing.\n",
    "<a id='4-1'></a>\n",
    "\n",
    "## 4.1. Padding Mask\n",
    "\n",
    "In natural language processing, input sequences often vary in length. To process such sequences with a Transformer model, we standardize their lengths. For instance, consider the following example with a maximum sequence length of five:\n",
    "\n",
    "```text\n",
    "[\n",
    "  [\"Do\", \"you\", \"know\", \"when\", \"Jane\", \"is\", \"going\", \"to\", \"visit\", \"Africa\"], \n",
    "  [\"Jane\", \"visits\", \"Africa\", \"in\", \"September\"],\n",
    "  [\"Exciting\", \"!\"]\n",
    "]\n",
    "```\n",
    "\n",
    "After tokenization and vectorization, the sequences might appear as follows:\n",
    "\n",
    "```text\n",
    "[\n",
    "  [71, 121, 4, 56, 99, 2344, 345, 1284, 15],\n",
    "  [56, 1285, 15, 181, 545],\n",
    "  [87, 600]\n",
    "]\n",
    "```\n",
    "\n",
    "To feed such sequences into a Transformer model, they must be standardized to a uniform length. This can be achieved by padding shorter sequences with zeros and truncating longer sequences to fit the maximum length:\n",
    "\n",
    "```text\n",
    "[\n",
    "  [71, 121, 4, 56, 99],       # Truncated\n",
    "  [2344, 345, 1284, 15, 0],   # Padded\n",
    "  [56, 1285, 15, 181, 545],   \n",
    "  [87, 600, 0, 0, 0]          # Padded\n",
    "]\n",
    "```\n",
    "\n",
    "Here, sequences exceeding the maximum length of five are truncated, and zeros are added to the shorter sequences. The attention mechanism assigns weights to all tokens in a sequence using the softmax function.However, these padding zeros can interfere with the softmax computation by artificially influencing the attention mechanism. Padding zeros, if left unhandled, will affect these weights, causing the model to attend to irrelevant positions.\n",
    "\n",
    "To mitigate this, we employ a **padding mask**, which is a boolean array specifying which sequence elements should be attended to ($1$) and which should be ignored ($0$). For example, the padding mask for the sequence `[87, 600, 0, 0, 0]` is `[1, 1, 0, 0, 0]`.\n",
    "\n",
    "- `$1$` indicates meaningful tokens.\n",
    "- `$0$` denotes padding positions.\n",
    "\n",
    "To prevent padded elements from influencing the softmax computation, we modify the sequence before applying softmax using the padding mask. Padding positions are assigned a value close to negative infinity ($-10^9$), ensuring that their softmax contribution becomes negligible. The transformed sequence becomes:\n",
    "\n",
    "```text\n",
    "[87, 600, -10^9, -10^9, -10^9]\n",
    "```\n",
    "\n",
    "When the softmax function processes this transformed sequence, the weights for positions corresponding to $-10^9$ become effectively zero, ensuring the attention mechanism focuses only on meaningful tokens. This adjustment ensures that the softmax function focuses only on meaningful sequence elements and ignores the padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(decoder_token_ids):\n",
    "    \"\"\"\n",
    "    Creates a padding mask for attention computations.\n",
    "    \n",
    "    Arguments:\n",
    "        decoder_token_ids -- Tensor of shape (n, m) representing input sequences.\n",
    "    \n",
    "    Returns:\n",
    "        mask -- Tensor of shape (n, 1, m) with binary values.\n",
    "    \"\"\"    \n",
    "    # Identify padding tokens (zeros) and create a mask\n",
    "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
    "    \n",
    "    # Add an extra dimension to the mask\n",
    "    return seq[:, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. 1. 0. 0. 1.]]\n",
      "\n",
      " [[1. 1. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1. 1.]]], shape=(3, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[7., 6., 0., 0., 1.], [1., 2., 3., 0., 0.], [0., 0., 0., 4., 5.]])\n",
    "print(create_padding_mask(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting padding mask identifies the positions of meaningful elements in each sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By multiplying $(1 - \\text{mask})$ with $-10^9$ and adding it to the input sequence, we effectively set padding values to negative infinity. Below is the softmax comparison:\n",
    "\n",
    "- **Original sequence**: Padding influences the computation, leading to incorrect attention weights.\n",
    "- **Masked sequence**: The padding elements are ignored, ensuring accurate attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[7.2876644e-01 2.6809821e-01 6.6454901e-04 6.6454901e-04 1.8064314e-03]\n",
      " [8.4437378e-02 2.2952460e-01 6.2391251e-01 3.1062774e-02 3.1062774e-02]\n",
      " [4.8541026e-03 4.8541026e-03 4.8541026e-03 2.6502505e-01 7.2041273e-01]], shape=(3, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[7.2973627e-01 2.6845497e-01 0.0000000e+00 0.0000000e+00 1.8088354e-03]\n",
      "  [2.4472848e-01 6.6524094e-01 0.0000000e+00 0.0000000e+00 9.0030573e-02]\n",
      "  [6.6483542e-03 6.6483542e-03 0.0000000e+00 0.0000000e+00 9.8670328e-01]]\n",
      "\n",
      " [[7.3057157e-01 2.6876226e-01 6.6619506e-04 0.0000000e+00 0.0000000e+00]\n",
      "  [9.0030573e-02 2.4472848e-01 6.6524094e-01 0.0000000e+00 0.0000000e+00]\n",
      "  [3.3333331e-01 3.3333331e-01 3.3333331e-01 0.0000000e+00 0.0000000e+00]]\n",
      "\n",
      " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6894143e-01 7.3105860e-01]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 4.9999997e-01 4.9999997e-01]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6894143e-01 7.3105860e-01]]], shape=(3, 3, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.activations.softmax(x))\n",
    "print(tf.keras.activations.softmax(x + (1 - create_padding_mask(x)) * -1.0e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frameworks such as Keras integrate this masking logic seamlessly in layers like `MultiHeadAttention`, ensuring that padding does not distort the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-2'></a>\n",
    "\n",
    "## 4.2. Look-ahead Mask\n",
    "\n",
    "The **look-ahead mask** (also known as the **causal mask**) is an essential component in the Transformer architecture, particularly during training when we want to prevent the model from accessing information it shouldn't use.\n",
    "\n",
    "<a id='4-2-1'></a>\n",
    "\n",
    "### 4.2.1. Purpose of the Look-ahead Mask\n",
    "\n",
    "In a typical autoregressive model, the model predicts the next token based on previous tokens, without \"looking ahead\" at future tokens. During training, however, we often have access to the entire correct output sequence. The look-ahead mask ensures that the model simulates a real-world prediction scenario by masking future tokens, thereby forcing the model to make predictions based solely on the tokens it has already seen.\n",
    "\n",
    "In simpler terms, the look-ahead mask prevents the model from cheating by looking at future tokens during training. The goal is for the model to learn to predict each token in the sequence based only on the preceding ones, similar to how it would operate during inference (when generating tokens one by one).\n",
    "\n",
    "#### Example Scenario\n",
    "\n",
    "Consider a sequence where the expected correct output is:\n",
    "\n",
    "```text\n",
    "[1, 2, 3]\n",
    "```\n",
    "\n",
    "When training the model, we want to test if the model can predict the second value i.e. the sequence `[1, 2, -1e9]`, given that it has already predicted the first one i.e. the sequence `[1, -1e9, -1e9]`. To do this, we mask out the second and third values and input the sequence `[1, -10^9, -10^9]`. This means the model can only \"see\" the first value during its prediction process for the second token. \n",
    "\n",
    "Similarly, for predicting the third value, we would mask out the second and third values during the prediction of the third token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(sequence_length):\n",
    "    \"\"\"\n",
    "    Constructs a lower triangular mask matrix for sequence modeling.\n",
    "    \n",
    "    This mask is typically used in transformer-based architectures to prevent a \n",
    "    position in a sequence from attending to subsequent positions, thereby enforcing \n",
    "    an autoregressive property.\n",
    "\n",
    "    Parameters:\n",
    "        sequence_length (int): The size of the square mask matrix.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: A tensor of shape (1, sequence_length, sequence_length) \n",
    "        containing the look-ahead mask, where positions in the upper triangle \n",
    "        (future positions) are zeroed out.\n",
    "    \"\"\"\n",
    "    # tf.linalg.band_part(input, -1, 0) ==> Lower triangular part including the main diagonal.\n",
    "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\n",
       "array([[[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "<h1 style=\"background: #FFC07F; border: 0; color: #2F2E41; \n",
    "    box-shadow: 4px 4px 8px rgba(0, 0, 0, 0.3); \n",
    "    padding: 10px; border-radius: 10px; margin: 15px 0;\">\n",
    "    <center style=\"color: #2F2E41;\">5. Self-Attention</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **self-attention mechanism**, introduced in the paper *\"Attention is All You Need\"*, is central to Transformer models. It allows them to process sequences of tokens efficiently and focus on the most relevant parts of the input for a given task. This detailed breakdown explores the theoretical foundation, step-by-step implementation, and its critical role in modern machine learning.\n",
    "\n",
    "<a id='5-1'></a>\n",
    "\n",
    "## 5.1. Purpose and Objective \n",
    "\n",
    "Self-attention dynamically determines the importance of each token in a sequence relative to others. Its main goals are:  \n",
    "1. **Contextual Understanding**: It captures relationships between tokens in a sequence, irrespective of their distance from each other. This is especially valuable for long-range dependencies, where traditional models like RNNs struggle.  \n",
    "2. **Parallel Processing**: Unlike RNNs, which process sequences one token at a time, self-attention enables simultaneous computation for all tokens, leading to significant speed-ups.  \n",
    "3. **Scalability**: By leveraging parallelization, self-attention scales well to longer sequences and larger datasets.\n",
    "\n",
    "For example, in a sentence like *\"The cat sat on the mat\"*, self-attention enables the model to focus on the relationship between \"cat\" and \"sat\" to infer meaning, while also understanding how \"mat\" relates to the overall sentence structure.\n",
    "\n",
    "<a id='5-2'></a>\n",
    "\n",
    "## 5.2. Mathematical Formulation\n",
    "\n",
    "The **scaled dot product attention** mechanism is at the core of self-attention.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/umermjd11/DLC5M4A1/master/images/self-attention.png\" alt=\"Encoder\">\n",
    "  <br>\n",
    "  <caption>\n",
    "    <font color=\"#00796b\" style=\"font-size: 1.2em; font-weight: bold;\">\n",
    "      <b>Figure 1</b>: Visualization of Self-Attention Calculation\n",
    "    </font>\n",
    "  </caption>\n",
    "</div>\n",
    "\n",
    "This mechanism takes in a query, key, value, and an optional mask as inputs, and computes rich, attention-based vector representations for the words in a sequence. and is mathematically represented as:  \n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M \\right)V\n",
    "$$ \n",
    "\n",
    "Where:  \n",
    "- **$Q$ (Query)**: Embedding of the current token asking for information.  \n",
    "- **$K$ (Key)**: Embeddings of all tokens, used to determine relevance to the query.  \n",
    "- **$V$ (Value)**: Actual content or information associated with each token.  \n",
    "- **$M$ (Mask)**: Optional mask to exclude irrelevant positions (e.g., padding or future tokens).  \n",
    "- **$d_k$**: Dimensionality of the key vectors, used for scaling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-3'></a>\n",
    "\n",
    "## 5.3. **Deep Dive into Self-Attention: Understanding Q, K, V, M, and $d_k$**\n",
    "\n",
    "In the self-attention mechanism of transformers, several key components play crucial roles in determining how each word (or token) in a sequence attends to other words. The components include the **Query (Q)**, **Key (K)**, **Value (V)** vectors, **Masking (M)**, and the **dimension of the keys and queries ($d_k$)**. Understanding these concepts individually and together is essential for grasping how transformers process sequences.\n",
    "\n",
    "Let’s break each down with examples, visualizations, and intuitive explanations:\n",
    "\n",
    "<a id='5-3-1'></a>\n",
    "\n",
    "### 5.3.1. **Query Matrix** ($Q$)\n",
    "\n",
    "The **query matrix ($Q$)** in self-attention represents a collection of **query vectors**, each associated with a token in the input sequence. A **query** is essentially the **embedding of the current token**, formulated to ask, **\"What information is relevant to me from the rest of the sequence?\"**\n",
    "\n",
    "#### 5.3.1.1. **Role Q (Query)**\n",
    "\n",
    "The **Query (Q)** vector represents the information that a specific token (word) is \"asking about\" the other tokens in the sequence. It is a query for relevance or \"attention\" to the other words based on their context.\n",
    "\n",
    "#### 5.3.1.2. **Purpose of Q:**\n",
    "- The primary function of the query is to **interact** with the **key (K)** vectors of all the words in the sequence, helping the model **determine the attention** it should give to each word.\n",
    "- The query seeks to find out how much attention each word in the sequence deserves in relation to the current token, which is represented by the query.\n",
    "- Once the relevance scores (attention weights) are calculated, these weights are used to perform a **weighted sum** of the **value (V)** vectors, which ultimately forms the output for the given token.\n",
    "\n",
    "In simple terms, the query determines how much a word should \"pay attention\" to the others based on their relevance.\n",
    "\n",
    "#### 5.3.1.3. **Example:**\n",
    "\n",
    "Let’s consider a simple sentence:\n",
    "\n",
    "**\"The cat sat on the mat.\"**\n",
    "\n",
    "We’ll focus on the word **\"cat\"** and examine how the query works:\n",
    "\n",
    "- The **Query for the word \"cat\"** is created from the word's embedding and transformed by a learned weight matrix $W_Q$.\n",
    "- The query vector will be compared to the key vectors of all the other words (including itself) to calculate the attention scores.\n",
    "- Based on the attention scores, the word \"cat\" will then attend more to certain words that are contextually relevant (e.g., \"sat\", \"mat\") and less to others (e.g., \"the\").\n",
    "\n",
    "#### 5.3.1.4. **Intuition Behind Q**\n",
    "\n",
    "To gain intuition about **Q**, let’s consider the following:\n",
    "\n",
    "##### **Query as a Request for Information:**\n",
    "- Imagine you're trying to answer a question, and the words in the sentence are the potential sources of information.\n",
    "- The word **\"cat\"** is **asking a question**: \"What information do I need from the rest of the words in the sentence?\" This question is represented by the query vector **Q**.\n",
    "- The query is **not static**: it depends on the context (the word itself), and how much attention the word gives to others is determined by its query vector interacting with the key vectors of the other words.\n",
    "\n",
    "##### **Query-Weighting Interaction:**\n",
    "- The query interacts with the **key vectors** of the other words using a similarity measure (usually the dot product). The result of this interaction gives a score, which tells us **how relevant the other word is to the current word**.\n",
    "- For example, the word **\"cat\"** might give higher attention to the word **\"sat\"** (since they’re related) and less attention to the word **\"on\"** (which is less semantically relevant).\n",
    "\n",
    "#### 5.3.1.5. **Mathematics of Query (Q)**\n",
    "\n",
    "Let’s delve into the mathematics to understand how the query vector works:\n",
    "\n",
    "##### **1. Embedding of Words**:\n",
    "Each word in the sequence is initially represented by an embedding vector, say **$X_{\\text{cat}}$** for the word \"cat\". This embedding is typically of high dimensionality (e.g., 512 or 1024), and it represents the token's position in the high-dimensional vector space of the model.\n",
    "\n",
    "##### **2. Query Vector Calculation**:\n",
    "The embedding for each word is transformed into the query vector by multiplying it with a learned weight matrix **$W_Q$**. For the word **\"cat\"**, the query vector **$Q_{\\text{cat}}$** is given by:\n",
    "\n",
    "$$\n",
    "Q_{\\text{cat}} = X_{\\text{cat}} \\cdot W_Q\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$X_{\\text{cat}}$** is the embedding vector for \"cat\".\n",
    "- **$W_Q$** is the weight matrix for the query transformation, learned during training.\n",
    "\n",
    "##### **3. Attention Score Calculation**:\n",
    "Once the query vector **$Q_{\\text{cat}}$** is obtained, it is compared with the **key vectors (K)** of all the words in the sequence using a similarity measure (typically the dot product):\n",
    "\n",
    "$$\n",
    "\\text{Attention Score}_{\\text{cat}, i} = Q_{\\text{cat}} \\cdot K_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$Q_{\\text{cat}}$** is the query vector for \"cat\".\n",
    "- **$K_i$** is the key vector for the $i$-th word in the sequence.\n",
    "- **$\\cdot$** denotes the dot product, which measures the similarity between the query and key vectors.\n",
    "\n",
    "This dot product will yield a scalar value representing the similarity between the current word (\"cat\") and each of the other words in the sequence.\n",
    "\n",
    "##### **4. Softmax and Normalization**:\n",
    "The attention scores are passed through a **softmax function** to normalize them into a probability distribution (i.e., the attention weights). This makes sure the attention scores sum to 1.\n",
    "\n",
    "$$\n",
    "\\text{Attention Weight}_{\\text{cat}, i} = \\frac{\\exp(\\text{Attention Score}_{\\text{cat}, i})}{\\sum_j \\exp(\\text{Attention Score}_{\\text{cat}, j})}\n",
    "$$\n",
    "\n",
    "This step ensures that each attention weight represents the importance of the corresponding word relative to the word \"cat\".\n",
    "\n",
    "\n",
    "#### 5.3.1.6. **Example Calculation with Attention Weights**\n",
    "\n",
    "Let’s assume the embeddings are simple and the transformation matrices are also small for the sake of clarity:\n",
    "\n",
    "- Embedding for **\"cat\"**: $X_{\\text{cat}} = [0.2, 0.7]$\n",
    "- Weights for queries: $W_Q = \\begin{bmatrix} 0.5 & 0.2 \\\\ 0.1 & 0.4 \\end{bmatrix}$\n",
    "\n",
    "##### Query Vector for \"cat\":\n",
    "$$\n",
    "Q_{\\text{cat}} = X_{\\text{cat}} \\cdot W_Q = \\begin{bmatrix} 0.2 & 0.7 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.5 & 0.2 \\\\ 0.1 & 0.4 \\end{bmatrix} = [0.23, 0.38]\n",
    "$$\n",
    "\n",
    "Now let’s assume the key vectors for \"sat\" and \"on\" are:\n",
    "\n",
    "- $K_{\\text{sat}} = [0.5, 0.3]$\n",
    "- $K_{\\text{on}} = [0.6, 0.1]$\n",
    "\n",
    "##### Attention Scores (Dot Product):\n",
    "- Attention score for \"cat\" and \"sat\":\n",
    "$$\n",
    "\\text{Attention Score}_{\\text{cat}, \\text{sat}} = Q_{\\text{cat}} \\cdot K_{\\text{sat}} = [0.23, 0.38] \\cdot [0.5, 0.3] = (0.23 \\times 0.5) + (0.38 \\times 0.3) = 0.115 + 0.114 = 0.229\n",
    "$$\n",
    "\n",
    "- Attention score for \"cat\" and \"on\":\n",
    "$$\n",
    "\\text{Attention Score}_{\\text{cat}, \\text{on}} = Q_{\\text{cat}} \\cdot K_{\\text{on}} = [0.23, 0.38] \\cdot [0.6, 0.1] = (0.23 \\times 0.6) + (0.38 \\times 0.1) = 0.138 + 0.038 = 0.176\n",
    "$$\n",
    "\n",
    "##### Softmax (Normalization of Attention Scores):\n",
    "Let’s compute the attention weights using softmax:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}([0.229, 0.176]) = \\frac{\\exp(0.229)}{\\exp(0.229) + \\exp(0.176)} \\approx [0.552, 0.448]\n",
    "$$\n",
    "\n",
    "These values indicate that \"cat\" will pay about **55.2%** attention to \"sat\" and **44.8%** attention to \"on\".\n",
    "\n",
    "#### 5.3.1.7. **Intuition Summary**\n",
    "\n",
    "- **Q** represents a **query** for the relevant context that the current token (\"cat\") is seeking from other tokens in the sequence.\n",
    "- It works by interacting with the **key vectors (K)** to calculate **attention scores** (using similarity measures such as dot product).\n",
    "- The attention scores are then normalized using **softmax** to create **attention weights**.\n",
    "- The weighted sum of the **value vectors (V)** corresponding to these weights produces the output, allowing the model to attend to the most relevant words in the sequence.\n",
    "\n",
    "Thus, **Q** enables each word in a sequence to dynamically adjust how much attention it pays to other words based on the context, and it is a key part of the self-attention mechanism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-3-2'></a>\n",
    "\n",
    "### 5.3.2. **Key Matrix** ($K$)\n",
    "\n",
    "In the **self-attention** mechanism, the **Key Matrix (K)** and the **key vectors** play a crucial role in helping the model determine how much attention each word (token) should pay to other words in the sequence. \n",
    "\n",
    "In the **self-attention** mechanism, the **Key (K)** is a learned transformation of the **embedding** of a word (token). It serves as the reference point for each word that will be used to calculate how relevant it is to a given query (Q).\n",
    "\n",
    "- The key vectors are used to **measure similarity** with the **query vector (Q)**. The dot product between the query and key vectors is the fundamental operation in calculating the attention scores, which represent the relevance between the query and the key.\n",
    "\n",
    "Let’s break down the **role**, **purpose**, **example**, **intuition**, **mathematics**, and provide a **mathematical example** for better understanding.\n",
    "\n",
    "#### 5.3.2.1. **Role of K (Key Matrix)**\n",
    "\n",
    "The **Key Matrix (K)** is the collection of key vectors for each word in the sequence. It is used to calculate how much attention each word should give to every other word based on their relationships.\n",
    "\n",
    "- **Defines how \"important\" a word is in relation to the query**: When a query vector interacts with a key vector, it determines how relevant one word is to another. The greater the similarity between the query and key vectors, the more attention the word should give to the other word.\n",
    "- **Facilitates attention score calculation**: The dot product between a **query vector (Q)** and a **key vector (K)** computes a score that determines how much attention should be given to the key's corresponding value vector (V).\n",
    "\n",
    "#### 5.3.2.2. **Purpose of Key (K)**\n",
    "\n",
    "- **Similarity measurement**: The key vector helps measure the similarity between different words in the sequence, enabling the model to calculate attention scores.\n",
    "- **Contextualization**: By comparing the query with the key, the model can determine which words are more important to attend to for context. This helps the model focus on the most relevant tokens and ignore irrelevant ones.\n",
    "\n",
    "#### 5.3.2.3. **Example of Key (K) and Keys in Action**\n",
    "\n",
    "Let’s take a simple sentence:  \n",
    "**\"The cat sat on the mat.\"**\n",
    "\n",
    "We’ll focus on the word **\"cat\"** and explore how the **key** works:\n",
    "\n",
    "- The **Key for the word \"cat\"** is derived from the word’s embedding and transformed by a learned weight matrix **$W_K$**. So, **$K_{\\text{cat}}$** represents the key for the word \"cat\".\n",
    "- The key vector is compared with the **Query vectors** of all words in the sequence to calculate **attention scores**.\n",
    "\n",
    "In this example, the word **\"cat\"** might give higher attention to the words **\"sat\"** and **\"mat\"**, and less attention to **\"the\"**. The keys for these words will be compared to the query vector of \"cat\" to calculate these attention weights.\n",
    "\n",
    "#### 5.3.2.4. **Intuition Behind Keys**\n",
    "\n",
    "To understand the intuition behind **key vectors (K)**, let’s think of **K** as **contextual information**.\n",
    "\n",
    "##### **Keys as Contextual Representations:**\n",
    "- Imagine a scenario where **\"cat\"** wants to understand the context of the sentence. Each word in the sequence has a corresponding **key vector**, which represents its importance in relation to other words.\n",
    "- The **key vector** serves as a **reference point** for comparison, while the **query vector** (Q) determines how relevant that key is to the current word (in this case, \"cat\").\n",
    "- The greater the similarity between a query and a key, the more relevant the associated word’s value vector will be for updating the current word's representation.\n",
    "\n",
    "##### **Key and Query Interaction:**\n",
    "- The **key vector** provides the \"reference\" to the query, and the **query vector** asks how relevant a particular word (represented by its key) is to the word generating the query.\n",
    "- A key with a high similarity to a query indicates that the corresponding word provides valuable contextual information.\n",
    "\n",
    "#### 5.3.2.5. **Mathematics of Key (K)**\n",
    "\n",
    "##### **1. Embedding of Words**:\n",
    "Each word in the sentence has an initial embedding. For example, let the word **\"cat\"** have an embedding **$X_{\\text{cat}}$**. The embedding vector is then transformed into a key vector by multiplying it by a learned weight matrix **$W_K$**.\n",
    "\n",
    "For the word \"cat\":\n",
    "$$\n",
    "K_{\\text{cat}} = X_{\\text{cat}} \\cdot W_K\n",
    "$$\n",
    "Where:\n",
    "- **$X_{\\text{cat}}$** is the embedding vector of the word \"cat\".\n",
    "- **$W_K$** is the weight matrix that transforms the embedding into a key vector.\n",
    "\n",
    "##### **2. Attention Score Calculation**:\n",
    "Once the key vectors are created for each word, the attention score is computed by calculating the **dot product** between the query vector and the key vectors. This dot product measures how similar the query is to each key.\n",
    "\n",
    "The attention score between a query **$Q_{\\text{cat}}$** and a key **$K_{\\text{sat}}$** would be:\n",
    "\n",
    "$$\n",
    "\\text{Attention Score}_{\\text{cat}, \\text{sat}} = Q_{\\text{cat}} \\cdot K_{\\text{sat}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$Q_{\\text{cat}}$** is the query vector for the word \"cat\".\n",
    "- **$K_{\\text{sat}}$** is the key vector for the word \"sat\".\n",
    "- **$\\cdot$** denotes the dot product between the query and the key.\n",
    "\n",
    "##### **3. Softmax and Normalization**:\n",
    "After calculating the attention scores, they are passed through a **softmax** function to normalize the scores into attention weights.\n",
    "\n",
    "$$\n",
    "\\text{Attention Weight}_{\\text{cat}, i} = \\frac{\\exp(\\text{Attention Score}_{\\text{cat}, i})}{\\sum_j \\exp(\\text{Attention Score}_{\\text{cat}, j})}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$i$** and **$j$** represent different words in the sequence.\n",
    "- The denominator ensures that the attention weights sum to 1.\n",
    "\n",
    "These attention weights tell us how much focus each word should have based on its key similarity to the query.\n",
    "\n",
    "#### 5.3.2.6. **Mathematical Example**\n",
    "\n",
    "Let’s continue with the example sentence **\"The cat sat on the mat\"**:\n",
    "\n",
    "- Suppose the embeddings for the words are simplified to 2D vectors:\n",
    "  - **$X_{\\text{cat}} = [0.2, 0.7]$**\n",
    "  - **$X_{\\text{sat}} = [0.5, 0.3]$**\n",
    "  - **$X_{\\text{on}} = [0.6, 0.1]$**\n",
    "\n",
    "Let’s assume the weight matrix for the keys **$W_K$** is:\n",
    "\n",
    "$$\n",
    "W_K = \\begin{bmatrix} 0.6 & 0.2 \\\\ 0.4 & 0.3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **Key Vector for \"cat\"**:\n",
    "$$\n",
    "K_{\\text{cat}} = X_{\\text{cat}} \\cdot W_K = [0.2, 0.7] \\cdot \\begin{bmatrix} 0.6 & 0.2 \\\\ 0.4 & 0.3 \\end{bmatrix} = [0.23, 0.38]\n",
    "$$\n",
    "\n",
    "##### **Key Vector for \"sat\"**:\n",
    "$$\n",
    "K_{\\text{sat}} = X_{\\text{sat}} \\cdot W_K = [0.5, 0.3] \\cdot \\begin{bmatrix} 0.6 & 0.2 \\\\ 0.4 & 0.3 \\end{bmatrix} = [0.36, 0.27]\n",
    "$$\n",
    "\n",
    "##### **Attention Score between \"cat\" and \"sat\"**:\n",
    "Now, let's calculate the attention score between **\"cat\"** and **\"sat\"** by taking the dot product of their query and key vectors.\n",
    "\n",
    "$$\n",
    "\\text{Attention Score}_{\\text{cat}, \\text{sat}} = Q_{\\text{cat}} \\cdot K_{\\text{sat}} = [0.23, 0.38] \\cdot [0.36, 0.27] = (0.23 \\times 0.36) + (0.38 \\times 0.27) = 0.0828 + 0.1026 = 0.1854\n",
    "$$\n",
    "\n",
    "##### **Softmax Calculation**:\n",
    "Let’s assume the attention scores for **\"cat\"** interacting with other words (e.g., \"sat\" and \"on\") are:\n",
    "\n",
    "- **Attention Score for \"sat\"**: 0.1854\n",
    "- **Attention Score for \"on\"**: 0.1602\n",
    "\n",
    "We apply **softmax** to normalize these scores into attention weights:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}([0.1854, 0.1602]) = \\frac{\\exp(0.1854)}{\\exp(0.1854) + \\exp(0.1602)} \\approx [0.507, 0.493]\n",
    "$$\n",
    "\n",
    "\n",
    "#### 5.3.2.7. **Intuition Summary**\n",
    "\n",
    "- **Key (K)** vectors represent the **contextual importance** of each word in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-3-3'></a>\n",
    "\n",
    "### 5.3.3. **Value Matrix** ($V$)\n",
    "\n",
    "In the **self-attention** mechanism, the **Value Matrix (V)** and the **value vectors** serve as the final pieces of the puzzle that produce the updated representations of words in the sequence. These vectors are the ones weighted according to the attention scores, resulting in a new context-aware representation of each word.\n",
    "\n",
    "The **Value Matrix (V)** contains the value vectors for each word in the sequence. These value vectors represent the **information** that will be passed on to the final output of the attention mechanism.\n",
    "\n",
    "- The value vectors are **weighted** by the attention scores (which are derived from the similarity between the query and key vectors).\n",
    "- After the attention scores are computed and normalized, each value vector is weighted according to its corresponding attention score, and the results are summed up to form the output.\n",
    "\n",
    "The **Value Matrix (V)** is often used in combination with the **Query Matrix (Q)** and **Key Matrix (K)** in the self-attention mechanism to capture the interactions and dependencies between words in the sequence.\n",
    "\n",
    "Let’s break down the **role**, **purpose**, **example**, **intuition**, **mathematics**, and provide a **mathematical example** to enhance your understanding of how the **Value Matrix (V)** works.\n",
    "\n",
    "#### 5.3.3.1. **Role of V (Value Matrix)**\n",
    "\n",
    "The **Value Matrix (V)** is crucial because it contains the information that will be attended to and passed along. It is essentially the source of the information that the attention mechanism uses to update the representation of each word.\n",
    "\n",
    "##### **Role of Value (V):**\n",
    "- **Carries information**: Each value vector contains the actual information about the word that will be passed along to the attention output.\n",
    "- **Weighted aggregation**: The value vectors are aggregated according to the attention scores calculated by the interaction between the query and key vectors. This allows the model to focus on the most relevant words and their corresponding values.\n",
    "\n",
    "#### 5.3.3.2. **Purpose of Value (V)**\n",
    "\n",
    "The purpose of the **value vectors** in the self-attention mechanism is to ensure that the final output for each word is based on a **weighted combination of the values** from all the words in the sequence. This allows the model to dynamically adjust which words contribute to the representation of a given word.\n",
    "\n",
    "##### **Purpose of the Value Matrix (V):**\n",
    "- **Contextualize the output**: The value vectors hold the information that will be passed on to the next layer of the model, ensuring that the output is contextually enriched.\n",
    "- **Enable focused learning**: By weighting the value vectors according to attention scores, the model learns to focus on the most relevant parts of the sequence, allowing for more meaningful representations of each word.\n",
    "\n",
    "#### 5.3.3.3. **Example of Value (V) and Values in Action**\n",
    "\n",
    "Let’s take a simple sentence again:  \n",
    "**\"The cat sat on the mat.\"**\n",
    "\n",
    "Let’s focus on the word **\"cat\"** and explore how the **value matrix (V)** works:\n",
    "\n",
    "- The **Value for the word \"cat\"** is derived from the word’s embedding and transformed by a learned weight matrix **$W_V$**. So, **$V_{\\text{cat}}$** represents the value for the word \"cat\".\n",
    "- Each word in the sequence will have its corresponding value vector (for example, **$V_{\\text{sat}}$**, **$V_{\\text{on}}$**, etc.).\n",
    "- The attention mechanism uses the **attention scores** (computed from the query-key interaction) to **weight the value vectors** of all words. These weighted value vectors are then summed to form the new representation for the word \"cat\".\n",
    "\n",
    "In this case, the word **\"cat\"** may focus on attending to words like **\"sat\"** and **\"mat\"**, while giving less attention to words like **\"the\"**. The weighted sum of the value vectors from these words will give the new representation for \"cat\".\n",
    "\n",
    "#### 5.3.3.4. **Intuition Behind Values**\n",
    "\n",
    "To understand the intuition behind **value vectors (V)**, let’s think of **V** as **the actual information** that is being passed between words in the sequence.\n",
    "\n",
    "##### **Values as Information Sources:**\n",
    "- Each word has a corresponding **value vector** that contains its information.\n",
    "- The **value vector** for a word is transformed from its embedding just like the key and query vectors.\n",
    "- The attention mechanism **weights** these value vectors based on the similarity of the query and key vectors. A high attention score means the corresponding value contributes more to the final output.\n",
    "\n",
    "##### **Value as a \"Content\" Carrier:**\n",
    "- While the **key** vector represents the **context** of a word, and the **query** vector represents the **current word's request**, the **value** vector is the **content** that is being passed around. This allows the attention mechanism to determine how much of the information from each word should influence the current word’s updated representation.\n",
    "\n",
    "### 5.3.3.5. **Mathematics of Value (V)**\n",
    "\n",
    "##### **1. Embedding of Words**:\n",
    "Each word in the sentence has an initial embedding. For example, let the word **\"cat\"** have an embedding **$X_{\\text{cat}}$**. The embedding vector is then transformed into a value vector by multiplying it by a learned weight matrix **$W_V$**.\n",
    "\n",
    "For the word \"cat\":\n",
    "$$\n",
    "V_{\\text{cat}} = X_{\\text{cat}} \\cdot W_V\n",
    "$$\n",
    "Where:\n",
    "- **$X_{\\text{cat}}$** is the embedding vector of the word \"cat\".\n",
    "- **$W_V$** is the weight matrix that transforms the embedding into a value vector.\n",
    "\n",
    "##### **2. Weighted Sum of Value Vectors**:\n",
    "After the attention scores are calculated (via the query-key interaction), each value vector is weighted by its corresponding attention score. The **weighted sum of value vectors** produces the final output for each query.\n",
    "\n",
    "The final output vector **$O_{\\text{cat}}$** for the word \"cat\" is the weighted sum of all value vectors:\n",
    "\n",
    "$$\n",
    "O_{\\text{cat}} = \\sum_i \\text{Attention Weight}_{\\text{cat}, i} \\cdot V_i\n",
    "$$\n",
    "Where:\n",
    "- **$O_{\\text{cat}}$** is the final output vector for the word \"cat\".\n",
    "- **$\\text{Attention Weight}_{\\text{cat}, i}$** is the attention weight for the **i-th** word in the sequence.\n",
    "- **$V_i$** is the value vector for the **i-th** word in the sequence.\n",
    "\n",
    "##### **3. Normalization**:\n",
    "To ensure the attention weights sum to 1, they are passed through the **softmax** function, as seen in the earlier explanation.\n",
    "\n",
    "#### 5.3.3.6. **Mathematical Example**\n",
    "\n",
    "Let’s continue with the example sentence **\"The cat sat on the mat\"**:\n",
    "\n",
    "- Suppose the embeddings for the words are simplified to 2D vectors:\n",
    "  - **$X_{\\text{cat}} = [0.2, 0.7]$**\n",
    "  - **$X_{\\text{sat}} = [0.5, 0.3]$**\n",
    "  - **$X_{\\text{on}} = [0.6, 0.1]$**\n",
    "\n",
    "Let’s assume the weight matrix for the values **$W_V$** is:\n",
    "\n",
    "$$\n",
    "W_V = \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.4 & 0.3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### **Value Vector for \"cat\"**:\n",
    "$$\n",
    "V_{\\text{cat}} = X_{\\text{cat}} \\cdot W_V = [0.2, 0.7] \\cdot \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.4 & 0.3 \\end{bmatrix} = [0.38, 0.26]\n",
    "$$\n",
    "\n",
    "##### **Value Vector for \"sat\"**:\n",
    "$$\n",
    "V_{\\text{sat}} = X_{\\text{sat}} \\cdot W_V = [0.5, 0.3] \\cdot \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.4 & 0.3 \\end{bmatrix} = [0.38, 0.21]\n",
    "$$\n",
    "\n",
    "##### **Value Vector for \"on\"**:\n",
    "$$\n",
    "V_{\\text{on}} = X_{\\text{on}} \\cdot W_V = [0.6, 0.1] \\cdot \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.4 & 0.3 \\end{bmatrix} = [0.34, 0.13]\n",
    "$$\n",
    "\n",
    "#### 5.3.3.7. **Intuition Summary**\n",
    "\n",
    "- **Value (V)** vectors contain the **information** that will be passed along to the final output.\n",
    "- **Value vectors** are **weighted** according to the attention scores that are based on the similarity between the query and key vectors.\n",
    "- **Weighted value vectors** are summed to produce the final output, ensuring that the most relevant words in the sequence contribute more to the updated representation of a word.\n",
    "- **The Value Matrix (V)** serves as the **content repository** for each word, while the attention mechanism helps decide how much of each word’s information should be passed forward.\n",
    "\n",
    "In essence, **Value (V)** is the **actual content** that gets passed through the model, modulated by how much **attention** each word receives based on the interactions between the query and the key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-3-4'></a>\n",
    "\n",
    "### 5.3.4. **Mask** ($M$)\n",
    "\n",
    "In the context of the **self-attention mechanism**, the **mask** plays a crucial role in controlling which parts of the sequence a word should attend to. It is especially important in tasks like **language modeling**, where we want to ensure that predictions at each position are based only on past or current information (not future information).\n",
    "\n",
    "A **mask** is a binary matrix (or tensor) applied to the attention scores or weights during the self-attention process. It ensures that certain positions (usually future words or irrelevant words) do not contribute to the attention calculation. In transformer models, there are two common types of masks:\n",
    "\n",
    "- **Padding Mask**: Ensures that padding tokens (used to make sequences of equal length) do not affect the attention calculation.\n",
    "- **Causal Mask** (or **Look-Ahead Mask**): Ensures that future positions cannot attend to the current position, which is crucial in autoregressive models like GPT.\n",
    "\n",
    "Let's break down the **role**, **purpose**, **example**, **intuition**, **mathematics**, and provide a **mathematical example** to illustrate how the **mask** operates in the attention mechanism.\n",
    "\n",
    "\n",
    "#### 5.3.4.1. **Role of Mask**\n",
    "\n",
    "The **mask** serves the role of **preventing illegal or unwanted interactions** in the self-attention mechanism. By doing so, it enables the model to:\n",
    "\n",
    "- **Prevent information leakage**: In tasks like autoregressive language modeling (e.g., GPT), a mask is used to ensure that the model cannot attend to future positions (words) while predicting the current word.\n",
    "- **Handle padding tokens**: In variable-length sequences, padding tokens are added to make all sequences the same length. A mask ensures that padding tokens are not attended to.\n",
    "- **Enforce causal relationships**: In sequence generation, the mask ensures that each word only attends to previous words (or the current word), and not the future words.\n",
    "\n",
    "#### 5.3.4.2. **Purpose of Mask**\n",
    "\n",
    "The primary purpose of the **mask** is to **control the flow of information** during the attention process. It enforces constraints on the attention mechanism to make sure that:\n",
    "\n",
    "1. **Information does not flow backward in time**: This is important for autoregressive models that need to predict words sequentially.\n",
    "2. **Padding tokens do not influence attention**: This ensures that padding does not corrupt the attention mechanism.\n",
    "\n",
    "In summary, the mask ensures that the attention mechanism works correctly by **guiding where attention is allowed to be applied**.\n",
    "\n",
    "#### 5.3.4.3. **Example of Mask in Action**\n",
    "\n",
    "Let’s take the following example sentence:\n",
    "\n",
    "**\"The cat sat on the mat.\"**\n",
    "\n",
    "Assume that we have the following **padding mask** for a sequence of length 6 (where the last token is padding):\n",
    "\n",
    "- **Sentence tokens**: [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "- **Padding token**: \"<PAD>\"\n",
    "\n",
    "We might pad the sequence to length 6 to make all sequences uniform. If we do this, the mask would look like this:\n",
    "\n",
    "| Token  | Mask Value |\n",
    "|--------|------------|\n",
    "| The    | 1          |\n",
    "| cat    | 1          |\n",
    "| sat    | 1          |\n",
    "| on     | 1          |\n",
    "| the    | 1          |\n",
    "| mat    | 1          |\n",
    "\n",
    "Now let’s assume we are performing **self-attention** on the word **\"cat\"**. The padding mask would ensure that if the sequence had padding tokens (e.g., \"<PAD>\"), they are not considered when calculating attention scores. \n",
    "\n",
    "For instance, if the sequence was longer and included padding tokens, we would mask out the padding positions (setting their attention scores to a very low value, typically negative infinity, so that their softmax results in zero).\n",
    "\n",
    "#### 5.3.4.4. **Intuition Behind Masking**\n",
    "\n",
    "The **mask** is intuitively like a **filter** or **gate** that controls which information is allowed to pass through the attention mechanism. \n",
    "\n",
    "- In **causal masking**, the filter ensures that the model does not \"peek ahead\" at future tokens. This is like looking at a sequence of words one by one, where each word can only know about the words before it (not after).\n",
    "- In **padding masking**, the filter ensures that padding tokens don’t interfere with the calculation of the attention scores, as they don’t carry meaningful information.\n",
    "\n",
    "##### **Example in Language Modeling**:\n",
    "\n",
    "- Imagine you're trying to predict the word **\"cat\"** in the sentence **\"The _cat_ sat on the mat\"**. Without masking, the model could potentially \"see\" the word **\"sat\"** (or even the future word **\"on\"**) while trying to predict **\"cat\"**, but this would violate the causal structure of language modeling. The **mask** ensures that the model only uses information from previous words (not future ones) to predict **\"cat\"**.\n",
    "\n",
    "\n",
    "#### 5.3.4.5. **Mathematics of Mask**\n",
    "\n",
    "In the self-attention mechanism, after calculating the attention scores (using the query and key vectors), the mask is applied to the attention matrix.\n",
    "\n",
    "##### **Step 1: Attention Scores Calculation**\n",
    "\n",
    "Let the attention scores for all query-key pairs be stored in a matrix **$A$**:\n",
    "\n",
    "$$\n",
    "A = Q \\cdot K^T\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$Q$** is the query matrix.\n",
    "- **$K$** is the key matrix.\n",
    "- The result **$A$** is an attention score matrix.\n",
    "\n",
    "##### **Step 2: Apply the Mask**\n",
    "\n",
    "Next, we apply the mask to this matrix **$A$**. The mask is usually a binary matrix where:\n",
    "\n",
    "- **0** indicates a blocked position (e.g., a future token or padding token).\n",
    "- **1** indicates an allowed position (e.g., a valid word in the sequence).\n",
    "\n",
    "To apply the mask, we add a very large negative number (such as **-∞**) to the positions where the mask value is **0**. This ensures that after applying softmax, the attention score for those positions becomes 0 (as **$e^{-\\infty} = 0$**).\n",
    "\n",
    "Let the mask be denoted as **$M$**, where:\n",
    "\n",
    "$$\n",
    "M = \\text{Mask matrix}\n",
    "$$\n",
    "\n",
    "To apply the mask to the attention matrix **$A$**, we perform element-wise addition:\n",
    "\n",
    "$$\n",
    "A_{\\text{masked}} = A + M\n",
    "$$\n",
    "\n",
    "##### **Step 3: Softmax and Final Attention Weights**\n",
    "\n",
    "Once the mask has been applied, we perform the **softmax** operation to get the final attention weights:\n",
    "\n",
    "$$\n",
    "\\text{Attention Weights} = \\text{softmax}(A_{\\text{masked}})\n",
    "$$\n",
    "\n",
    "This gives us the attention weights that respect the masking constraints (i.e., attention only to valid positions).\n",
    "\n",
    "#### 5.3.4.6. **Mathematical Example of Masking**\n",
    "\n",
    "Consider the sequence **\"The cat sat on the mat\"**:\n",
    "\n",
    "Let’s assume the **padding mask** looks like this (1 for valid tokens and 0 for padding tokens):\n",
    "\n",
    "| Token | Mask |\n",
    "|-------|------|\n",
    "| The   | 1    |\n",
    "| cat   | 1    |\n",
    "| sat   | 1    |\n",
    "| on    | 1    |\n",
    "| the   | 1    |\n",
    "| mat   | 1    |\n",
    "\n",
    "Now, imagine we have computed the attention scores matrix **$A$** before applying the mask. Suppose it looks like this:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "2.0 & 0.3 & 0.1 & -1.0 & 0.0 & 0.4 \\\\\n",
    "0.1 & 1.2 & 0.4 & 0.0 & 0.5 & 0.1 \\\\\n",
    "1.1 & 0.5 & 1.8 & 0.4 & 0.2 & 0.6 \\\\\n",
    "0.4 & 0.6 & 0.3 & 2.0 & 0.7 & 0.5 \\\\\n",
    "0.2 & 0.1 & 0.5 & 0.3 & 1.3 & 0.4 \\\\\n",
    "0.3 & 0.7 & 0.2 & 0.0 & 0.4 & 1.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, apply the **mask** to block certain positions. For example, let’s mask out the future words in a **causal mask**:\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix}\n",
    "0 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "0 & 0 & 1 & 1 & 1 & 1 \\\\\n",
    "0 & 0 & 0 & 1 & 1 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We then **add** the mask to the attention scores matrix **$A$** (in this case, we add large negative values for the invalid positions):\n",
    "\n",
    "$$\n",
    "A_{\\text{masked}} = A + M\n",
    "$$\n",
    "\n",
    "Finally, applying **softmax** to the matrix **$A_{\\text{masked}}$** will give the valid attention weights, ensuring that only valid (non-masked) positions contribute to the final attention calculation.\n",
    "\n",
    "#### 5.3.4.7. **Intuition Summary**\n",
    "\n",
    "- **Mask** ensures that the attention mechanism respects certain constraints (e.g., causal constraints, padding constraints).\n",
    "- It acts as a **filter** to\n",
    "\n",
    " allow only certain interactions, preventing illegal or unwanted interactions.\n",
    "- In **causal masking**, it prevents looking ahead at future tokens, while in **padding masking**, it ensures padding tokens don’t interfere with attention calculations.\n",
    "- The mask guides where the attention should focus, making sure the model attends only to valid positions based on the task requirements (e.g., sequence generation or handling padding).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-3-5'></a>\n",
    "\n",
    "### 5.3.5. **Dimensionality of Key Vectors** ($d_k$)\n",
    "\n",
    "In the self-attention mechanism, **$d_k$** refers to the **dimensionality of the key vectors**. It's a key hyperparameter that plays a crucial role in determining the scale of the dot products between query and key vectors. \n",
    "\n",
    "- **$d_k$** is the **dimensionality of the key vectors** in the self-attention mechanism.\n",
    "- It is the number of features or components in each key vector (and similarly, the query vector, since both queries and keys typically share the same dimensionality in most models).\n",
    "- If the query and key vectors are of length **$d_k$**, then the attention calculation involves the dot product between a query vector and each key vector.\n",
    "\n",
    "Let's explore the **role**, **purpose**, **example**, **intuition**, **mathematics**, and provide a **mathematical example** to illustrate its function.\n",
    "\n",
    "#### 5.3.5.1. **Role of $d_k$**\n",
    "\n",
    "In self-attention, the **dot product** between a query vector **$Q$** and a key vector **$K$** determines the **relevance** or **similarity** between the two vectors. The larger the dot product, the more relevant the key is to the query.\n",
    "\n",
    "However, the raw dot product can sometimes grow too large when the dimensions of the vectors **$Q$** and **$K$** are high. This can cause **gradient instability** during training, as very large values in the attention scores would lead to sharp gradients.\n",
    "\n",
    "To address this, **$d_k$** is used to **scale** the dot product. Specifically, the dot product is divided by the square root of **$d_k$** to reduce the magnitude of the attention scores, leading to better stability during training.\n",
    "\n",
    "#### 5.3.5.2. **Purpose of $d_k$**\n",
    "\n",
    "The main purpose of **$d_k$** is to control the **scaling of the dot product** between query and key vectors. Without this scaling, the dot product might become too large as the dimensionality increases, causing numerical instability. Scaling by **$\\sqrt{d_k}$** ensures that the attention scores remain within a reasonable range, promoting more stable training and convergence.\n",
    "\n",
    "#### 5.3.5.3. **Example of $d_k$**\n",
    "\n",
    "Consider a simple self-attention setup where we have the following:\n",
    "\n",
    "- **$d_k = 3$**, meaning each query and key vector is of length 3.\n",
    "- **Query vector**: $Q = [1, 2, 3]$\n",
    "- **Key vector**: $K = [2, 3, 4]$\n",
    "\n",
    "The **dot product** between the query and key would be:\n",
    "\n",
    "$$\n",
    "Q \\cdot K = (1 \\times 2) + (2 \\times 3) + (3 \\times 4) = 2 + 6 + 12 = 20\n",
    "$$\n",
    "\n",
    "If we didn't scale this dot product, it could get too large, especially if **$d_k$** was much larger. But by scaling it, we reduce the impact of the dimensionality on the attention scores.\n",
    "\n",
    "After scaling by **$\\sqrt{d_k} = \\sqrt{3} \\approx 1.732$**, the scaled dot product becomes:\n",
    "\n",
    "$$\n",
    "\\frac{Q \\cdot K}{\\sqrt{d_k}} = \\frac{20}{1.732} \\approx 11.54\n",
    "$$\n",
    "\n",
    "#### 5.3.5.4. **Intuition Behind $d_k$**\n",
    "\n",
    "**$d_k$** controls the **size of the attention scores**. If the query and key vectors are high-dimensional, their raw dot product can become excessively large, making the softmax function (which normalizes the attention scores) produce very sharp, almost binary outputs. This would mean the model is overly confident about its attention, leading to poor generalization during training.\n",
    "\n",
    "By introducing **$d_k$**, we normalize the dot product, making sure that the attention scores don't become too large as the dimensionality increases. This **scaling** ensures that the self-attention mechanism works effectively, even with high-dimensional vectors, and that the model can train more smoothly.\n",
    "\n",
    "#### 5.3.5.5. **Mathematics of $d_k$**\n",
    "\n",
    "In the self-attention mechanism, the query, key, and value vectors are matrices:\n",
    "\n",
    "- **$Q$**: Query matrix of shape **$(N, d_k)$** (where **$N$** is the number of tokens and **$d_k$** is the dimension of each query vector).\n",
    "- **$K$**: Key matrix of shape **$(N, d_k)$**.\n",
    "- **$V$**: Value matrix of shape **$(N, d_v)$** (where **$d_v$** is the dimension of the value vectors).\n",
    "\n",
    "The attention scores between each query and key are calculated as the **dot product**:\n",
    "\n",
    "$$\n",
    "A = Q \\cdot K^T\n",
    "$$\n",
    "\n",
    "Where **$A$** is an **$N \\times N$** attention score matrix, and **$K^T$** is the transpose of the key matrix.\n",
    "\n",
    "To ensure that the attention scores are stable, we **scale** the dot product by **$\\sqrt{d_k}$**:\n",
    "\n",
    "$$\n",
    "A = \\frac{Q \\cdot K^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "Then, a **softmax** operation is applied to the attention matrix to normalize the scores and produce the attention weights:\n",
    "\n",
    "$$\n",
    "\\text{Attention Weights} = \\text{softmax}(A)\n",
    "$$\n",
    "\n",
    "Finally, the output is computed by multiplying the attention weights by the value matrix:\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{Attention Weights} \\cdot V\n",
    "$$\n",
    "\n",
    "#### 5.3.5.6. **Mathematical Example**\n",
    "\n",
    "Let's assume the following:\n",
    "\n",
    "- **$d_k = 2$** (i.e., query and key vectors have 2 dimensions).\n",
    "- Query vector: $Q = [1, 2]$\n",
    "- Key vector: $K = [3, 4]$\n",
    "- Value vector: $V = [5, 6]$\n",
    "\n",
    "##### Step 1: Compute the Dot Product\n",
    "\n",
    "The dot product between **$Q$** and **$K$** is:\n",
    "\n",
    "$$\n",
    "Q \\cdot K = (1 \\times 3) + (2 \\times 4) = 3 + 8 = 11\n",
    "$$\n",
    "\n",
    "##### Step 2: Scale the Dot Product by $\\sqrt{d_k}$\n",
    "\n",
    "Since **$d_k = 2$**, we scale the dot product by **$\\sqrt{2}$**:\n",
    "\n",
    "$$\n",
    "\\frac{Q \\cdot K}{\\sqrt{d_k}} = \\frac{11}{\\sqrt{2}} \\approx \\frac{11}{1.414} \\approx 7.78\n",
    "$$\n",
    "\n",
    "##### Step 3: Apply Softmax (if applicable)\n",
    "\n",
    "If there are multiple keys, we would apply the **softmax** function to this score to obtain the attention weights. For simplicity, let's assume this is the only score.\n",
    "\n",
    "##### Step 4: Compute the Output\n",
    "\n",
    "Once the attention weights are obtained (via softmax), we compute the weighted sum of the values. If the softmax score is, say, **0.8**, the output would be:\n",
    "\n",
    "$$\n",
    "\\text{Output} = 0.8 \\times V = 0.8 \\times [5, 6] = [4, 4.8]\n",
    "$$\n",
    "\n",
    "#### 5.3.5.7. **Intuition Summary of $d_k$**\n",
    "\n",
    "- **$d_k$** is the dimensionality of the key vectors, controlling the scale of the dot product between queries and keys.\n",
    "- It ensures that the **dot product** doesn't grow too large as the dimensionality increases, maintaining stability during training.\n",
    "- By scaling the dot product by **$\\sqrt{d_k}$**, the model can handle high-dimensional vectors without the risk of excessively large attention scores.\n",
    "- **$d_k$** directly affects the stability and effectiveness of the self-attention mechanism, ensuring that the model can train smoothly and efficiently even with large vector dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-3-6'></a>\n",
    "\n",
    "### 5.3.6. Understanding **Attention(Q, K, V)** in the Context of Self-Attention Mechanism\n",
    "\n",
    "The **Attention** mechanism in neural networks, especially in transformer models, allows the model to focus on different parts of the input sequence when generating an output. It takes into account a **query** vector, a **key** vector, and a **value** vector, and computes an attention score that determines how much focus each word in the sequence should have on every other word.\n",
    "\n",
    "Let’s explore the **definition**, **role**, **purpose**, **example**, **intuition**, **maths**, and a **mathematical example** to explain how the Attention mechanism works.\n",
    "\n",
    "\n",
    "#### 5.3.6.1. **Definition of Attention(Q, K, V)**\n",
    "\n",
    "- **Attention** is a function that takes three inputs:\n",
    "  - **$Q$**: Query matrix or vector, which represents the current token asking for information.\n",
    "  - **$K$**: Key matrix or vector, which represents the potential tokens to be attended to.\n",
    "  - **$V$**: Value matrix or vector, which contains the actual information that is passed along once attention scores are computed.\n",
    "\n",
    "In the self-attention mechanism, the **Attention(Q, K, V)** function computes a set of attention scores based on the interaction between the query and the keys, and then produces a weighted sum of the corresponding values.\n",
    "\n",
    "#### 5.3.6.2. **Role of Attention(Q, K, V)**\n",
    "\n",
    "- The role of **Attention(Q, K, V)** is to compute the attention scores that determine how much attention each token should pay to the others.\n",
    "- This function helps a model decide which parts of the sequence are most important to focus on while generating a representation of each token.\n",
    "\n",
    "For example, in a sentence like \"The cat sat on the mat\", when encoding the word \"sat\", the attention mechanism helps the model decide which other words, like \"cat\" and \"mat\", are more important for generating the representation of \"sat\".\n",
    "\n",
    "\n",
    "#### 5.3.6.3. **Purpose of Attention(Q, K, V)**\n",
    "\n",
    "- The primary purpose of **Attention(Q, K, V)** is to allow the model to **dynamically adjust the weight** of the importance of each word based on the context of the sequence.\n",
    "- It helps in **contextualizing** each token’s representation by considering not only the token itself but also other tokens in the sequence that might be relevant.\n",
    "- In other words, it enables the model to focus more on important tokens (e.g., syntactic or semantic relationships) while ignoring less relevant ones.\n",
    "\n",
    "\n",
    "#### 5.3.6.4. **Example of Attention(Q, K, V)**\n",
    "\n",
    "Consider the following example:\n",
    "\n",
    "- Suppose we have the sentence: \"The cat sat on the mat.\"\n",
    "- To compute attention for the word \"sat\", the query is the vector representing \"sat\", the keys are the vectors representing all the words in the sentence, and the values are the vectors that contain the actual information (typically the same as the tokens' embeddings).\n",
    "\n",
    "We calculate how much attention \"sat\" should give to \"The\", \"cat\", \"on\", and \"the mat\".\n",
    "\n",
    "\n",
    "#### 5.3.6.5. **Intuition Behind Attention(Q, K, V)**\n",
    "\n",
    "The attention mechanism works by determining how much a **query vector** should \"attend\" to each of the **key vectors**. The **value vectors** are then weighted according to these attention scores, and the weighted sum is the output of the attention mechanism.\n",
    "\n",
    "- **Query-Key Interaction**: The interaction between a query and a key vector is measured using a **similarity function**, such as a dot product. The more similar the query and key, the higher the attention score.\n",
    "- **Softmax**: After computing the similarities (attention scores), we apply a **softmax function** to normalize the scores, making them sum to 1. This allows the scores to be interpreted as probabilities.\n",
    "- **Weighted Sum**: The attention scores are then used to compute a **weighted sum** of the value vectors. The higher the attention score, the more influence the corresponding value has on the final output.\n",
    "\n",
    "\n",
    "#### 5.3.6.6. **Mathematics of Attention(Q, K, V)**\n",
    "\n",
    "The steps involved in calculating **Attention(Q, K, V)** are:\n",
    "\n",
    "1. **Compute the Dot Product**: Compute the similarity between the query and key vectors by taking the dot product.\n",
    "   $$\n",
    "   \\text{dot product}(Q, K_i) = Q \\cdot K_i\n",
    "   $$\n",
    "   where **$Q$** is the query and **$K_i$** is the i-th key vector.\n",
    "\n",
    "2. **Scale the Dot Product**: To avoid large values, the dot product is scaled by **$\\sqrt{d_k}$**, where **$d_k$** is the dimensionality of the key vectors.\n",
    "   $$\n",
    "   \\text{scaled dot product} = \\frac{Q \\cdot K_i}{\\sqrt{d_k}}\n",
    "   $$\n",
    "\n",
    "3. **Apply Softmax**: Apply the softmax function to the scaled dot products to obtain the attention weights.\n",
    "   $$\n",
    "   \\alpha_i = \\text{softmax}\\left( \\frac{Q \\cdot K_i}{\\sqrt{d_k}} \\right)\n",
    "   $$\n",
    "   where **$\\alpha_i$** is the attention weight for the i-th key.\n",
    "\n",
    "4. **Compute the Weighted Sum**: Multiply the attention weights by the corresponding value vectors and sum them to get the output:\n",
    "   $$\n",
    "   \\text{Attention Output} = \\sum_i \\alpha_i \\cdot V_i\n",
    "   $$\n",
    "\n",
    "\n",
    "#### 5.3.6.7. **Mathematical Example of Attention(Q, K, V)**\n",
    "\n",
    "Let’s work through an example:\n",
    "\n",
    "- Let **$d_k = 2$** (dimensionality of the key vectors).\n",
    "- Suppose we have the following query and key vectors:\n",
    "\n",
    "  - Query vector: $Q = [1, 0]$\n",
    "  - Key vectors: $K_1 = [2, 1]$, $K_2 = [0, 1]$\n",
    "  - Value vectors: $V_1 = [1, 0]$, $V_2 = [0, 1]$\n",
    "\n",
    "#### Step 1: Compute the Dot Product\n",
    "\n",
    "First, compute the dot products between the query vector and each key vector:\n",
    "\n",
    "$$\n",
    "Q \\cdot K_1 = (1 \\times 2) + (0 \\times 1) = 2\n",
    "$$\n",
    "$$\n",
    "Q \\cdot K_2 = (1 \\times 0) + (0 \\times 1) = 0\n",
    "$$\n",
    "\n",
    "#### Step 2: Scale the Dot Product\n",
    "\n",
    "Since **$d_k = 2$**, we scale the dot products by **$\\sqrt{2} \\approx 1.414$**:\n",
    "\n",
    "$$\n",
    "\\frac{Q \\cdot K_1}{\\sqrt{d_k}} = \\frac{2}{1.414} \\approx 1.41\n",
    "$$\n",
    "$$\n",
    "\\frac{Q \\cdot K_2}{\\sqrt{d_k}} = \\frac{0}{1.414} = 0\n",
    "$$\n",
    "\n",
    "#### Step 3: Apply Softmax\n",
    "\n",
    "Next, apply softmax to the scaled dot products to get the attention weights:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}([1.41, 0]) = \\left[ \\frac{e^{1.41}}{e^{1.41} + e^{0}}, \\frac{e^{0}}{e^{1.41} + e^{0}} \\right] \\approx [0.80, 0.20]\n",
    "$$\n",
    "\n",
    "So, the attention weights are approximately **0.80** for the first key and **0.20** for the second key.\n",
    "\n",
    "#### Step 4: Compute the Weighted Sum\n",
    "\n",
    "Finally, compute the weighted sum of the value vectors:\n",
    "\n",
    "$$\n",
    "\\text{Attention Output} = (0.80 \\times V_1) + (0.20 \\times V_2)\n",
    "$$\n",
    "$$\n",
    "= (0.80 \\times [1, 0]) + (0.20 \\times [0, 1]) = [0.80, 0.20]\n",
    "$$\n",
    "\n",
    "So, the output of the attention mechanism is **[0.80, 0.20]**.\n",
    "\n",
    "\n",
    "#### 5.3.6.8. **Intuition Summary of Attention(Q, K, V)**\n",
    "\n",
    "- **Attention(Q, K, V)** computes a set of attention scores between the query and all keys.\n",
    "- It determines how much weight each word should assign to the others in a sequence based on their similarity (query-key interaction).\n",
    "- The softmax operation normalizes these attention scores so that they sum to 1, giving us probability-like weights.\n",
    "- The final output is a weighted sum of the value vectors, with higher attention scores giving more influence to the corresponding value vectors.\n",
    "- This allows the model to focus more on relevant tokens and less on irrelevant ones, enabling efficient context-based learning and representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Attention(Q, K, V)** vector is calculated for each position in the sequence, but the key idea is that each token (or position) in the sequence has its own **query** vector, and it interacts with **all the key vectors** to compute attention scores. Then, it produces a weighted sum of the **value** vectors based on these scores.\n",
    "\n",
    "<a id='5-3-7'></a>\n",
    "\n",
    "### 5.3.7. Attention Calculation for Each Position\n",
    "\n",
    "Let’s break this down:\n",
    "\n",
    "1. **For each token (or position) in the sequence**:\n",
    "   - You compute a **query** vector for that token (using the token’s embedding and possibly transformations through learned weights).\n",
    "   - You use this **query** vector to calculate the attention scores by interacting it with **all the key vectors** of all tokens in the sequence (including itself).\n",
    "   - The output for that token is a weighted sum of the **value vectors** based on these attention scores.\n",
    "\n",
    "2. **For the entire sequence**:\n",
    "   - The **Attention(Q, K, V)** calculation is done for each token in the sequence (i.e., for each query in the sequence).\n",
    "   - The result of this computation for each token will be a new vector, which encodes information from the entire sequence but weighted by the relevance of other tokens.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Let’s assume we have a sequence of three tokens (words) in the sentence:  \n",
    "\"**The cat sat**\"\n",
    "\n",
    "For each token:\n",
    "- **Query (Q)** will be computed for \"The\", \"cat\", and \"sat\" separately.\n",
    "- **Key (K)** and **Value (V)** matrices represent all tokens in the sequence (same for every token but different for each word's position).\n",
    "- For the word \"The\":\n",
    "  - The query vector for \"The\" will compute attention with the key vectors of all words in the sequence (\"The\", \"cat\", \"sat\").\n",
    "  - The result will be a weighted sum of the value vectors of all words, where the weights are determined by how \"relevant\" each word is to the word \"The\".\n",
    "\n",
    "#### Process:\n",
    "1. **Compute Query for \"The\"**:  \n",
    "   $Q_{\\text{The}}$\n",
    "   \n",
    "2. **Compute Attention Scores**:  \n",
    "   $Q_{\\text{The}} \\cdot K_{\\text{The}}$, $Q_{\\text{The}} \\cdot K_{\\text{cat}}$, $Q_{\\text{The}} \\cdot K_{\\text{sat}}$\n",
    "\n",
    "3. **Apply Softmax** to get attention weights.\n",
    "\n",
    "4. **Compute Weighted Sum**:  \n",
    "   Attention output for \"The\" = $\\alpha_{\\text{The}} \\cdot V_{\\text{The}} + \\alpha_{\\text{cat}} \\cdot V_{\\text{cat}} + \\alpha_{\\text{sat}} \\cdot V_{\\text{sat}}$\n",
    "\n",
    "Repeat the same process for the words \"cat\" and \"sat\".\n",
    "\n",
    "### Summary:\n",
    "- The **Attention(Q, K, V)** calculation is done for each position (or token) in the sequence.\n",
    "- Each token produces an attention output by querying all other tokens and using their respective values based on the computed attention scores.\n",
    "- The model can attend to different parts of the sequence for each token, allowing it to build contextualized representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Computes the scaled dot-product attention.\n",
    "\n",
    "    The function calculates the attention scores by computing the dot product of the query and key matrices, \n",
    "    scaling the result, and then applying a softmax function to obtain the attention weights. \n",
    "    These weights are then used to weight the values matrix, producing the final output.\n",
    "\n",
    "    Arguments:\n",
    "        q -- query tensor of shape (..., seq_len_q, depth)\n",
    "        k -- key tensor of shape (..., seq_len_k, depth)\n",
    "        v -- value tensor of shape (..., seq_len_v, depth_v)\n",
    "        mask -- Optional tensor, broadcastable to shape (..., seq_len_q, seq_len_k), used to mask certain positions (default: None)\n",
    "\n",
    "    Returns:\n",
    "        output -- Tensor of shape (..., seq_len_q, depth_v), weighted sum of values according to attention weights\n",
    "        attention_weights -- Tensor of shape (..., seq_len_q, seq_len_k), the attention weights corresponding to each query-key pair\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Compute the dot product of queries and transposed keys\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # shape: (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # 2. Scale the dot product by the square root of the depth of the key vectors (d_k)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)  # depth of the key (or value) vectors\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # 3. Apply the mask to the scaled attention logits (if provided)\n",
    "    if mask is not None:\n",
    "        # Add a large negative value to the masked positions so that they are ignored after softmax\n",
    "        scaled_attention_logits += (1 - mask) * -1.0e9\n",
    "\n",
    "    # 4. Apply softmax to the scaled logits to get the attention weights\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # Normalize along seq_len_k\n",
    "\n",
    "    # 5. Compute the final output by multiplying attention weights with the value matrix\n",
    "    output = tf.matmul(attention_weights, v)  # shape: (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting tests for TestScaledDotProductAttention...\n",
      "Running test_attention_shapes...\n",
      "test_attention_shapes: OK\n",
      "Running test_attention_with_mask...\n",
      "test_attention_with_mask: OK\n",
      "Running test_scaled_dot_product_attention...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.490s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed\n",
      "test_scaled_dot_product_attention: OK\n"
     ]
    }
   ],
   "source": [
    "# Unit Test Class for scaled_dot_product_attention\n",
    "class TestScaledDotProductAttention(unittest.TestCase):\n",
    "    \n",
    "    def test_scaled_dot_product_attention(self):\n",
    "        \"\"\"\n",
    "        Validate correctness using public test utility\n",
    "        \"\"\"\n",
    "        print(\"Running test_scaled_dot_product_attention...\")\n",
    "        try:\n",
    "            scaled_dot_product_attention_test(scaled_dot_product_attention)  # Calling the public test function\n",
    "            print(\"test_scaled_dot_product_attention: OK\")\n",
    "        except AssertionError as e:\n",
    "            print(\"test_scaled_dot_product_attention: FAIL\")\n",
    "            print(e)\n",
    "\n",
    "    def test_attention_shapes(self):\n",
    "        print(\"Running test_attention_shapes...\")\n",
    "        q = tf.random.normal(shape=(2, 5, 64))  # (batch_size, seq_len_q, depth)\n",
    "        k = tf.random.normal(shape=(2, 5, 64))  # (batch_size, seq_len_k, depth)\n",
    "        v = tf.random.normal(shape=(2, 5, 128)) # (batch_size, seq_len_v, depth_v)\n",
    "\n",
    "        try:\n",
    "            output, attention_weights = scaled_dot_product_attention(q, k, v, mask=None)\n",
    "            self.assertEqual(output.shape, (2, 5, 128))  # Expected shape: (batch_size, seq_len_q, depth_v)\n",
    "            self.assertEqual(attention_weights.shape, (2, 5, 5))  # Expected shape: (batch_size, seq_len_q, seq_len_k)\n",
    "            print(\"test_attention_shapes: OK\")\n",
    "        except AssertionError as e:\n",
    "            print(\"test_attention_shapes: FAIL\")\n",
    "            print(e)\n",
    "\n",
    "    def test_attention_with_mask(self):\n",
    "        print(\"Running test_attention_with_mask...\")\n",
    "        q = tf.random.normal(shape=(2, 5, 64))\n",
    "        k = tf.random.normal(shape=(2, 5, 64))\n",
    "        v = tf.random.normal(shape=(2, 5, 128))\n",
    "\n",
    "        mask = tf.constant([[1, 1, 0, 0, 0], [1, 0, 0, 1, 1]], dtype=tf.float32)\n",
    "        mask = mask[:, tf.newaxis, :]  # Reshape to (batch_size, 1, seq_len_k)\n",
    "\n",
    "        try:\n",
    "            output, attention_weights = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "            self.assertEqual(output.shape, (2, 5, 128))  # Expected shape: (batch_size, seq_len_q, depth_v)\n",
    "            self.assertEqual(attention_weights.shape, (2, 5, 5))  # Expected shape: (batch_size, seq_len_q, seq_len_k)\n",
    "            print(\"test_attention_with_mask: OK\")\n",
    "        except AssertionError as e:\n",
    "            print(\"test_attention_with_mask: FAIL\")\n",
    "            print(e)\n",
    "\n",
    "\n",
    "# Run the tests for `TestScaledDotProductAttention`\n",
    "def run_tests():\n",
    "    print(\"\\nStarting tests for TestScaledDotProductAttention...\")\n",
    "    unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestScaledDotProductAttention))\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just implemented self-attention. With that, we can start building the encoder block! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "<h1 style=\"background: #FFC07F; border: 0; color: #2F2E41; \n",
    "    box-shadow: 4px 4px 8px rgba(0, 0, 0, 0.3); \n",
    "    padding: 10px; border-radius: 10px; margin: 15px 0;\">\n",
    "    <center style=\"color: #2F2E41;\">6. Encoder</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Transformer Encoder** is a fundamental part of the Transformer architecture, which consists of several key components that work together to process and encode the input sequence. The Encoder uses self-attention mechanisms to capture the relationships between different words or tokens in the sequence, and then it applies additional processing to enhance the model's capabilities. \n",
    "\n",
    "The Transformer Encoder layer pairs self-attention and feedfarward neural network style of processing (Figure 2a) to improve the speed of training and passes K and V matrices to the Decoder.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/umermjd11/DLC5M4A1/master/images/encoder_layer.png\" alt=\"Encoder\" width=\"250\"/>\n",
    "  <br>\n",
    "  <caption>\n",
    "    <font color=\"#00796b\" style=\"font-size: 1.2em; font-weight: bold;\">\n",
    "      <b>Figure 2a</b>: Transformer encoder layer\n",
    "    </font>\n",
    "  </caption>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='6-1'></a>\n",
    "\n",
    "## 6.1. **Multi-Head Attention**\n",
    "\n",
    "**Multi-Head Attention** is a crucial mechanism in the Transformer Encoder, allowing the model to learn multiple attention patterns simultaneously. The input sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. \n",
    "\n",
    "- **Purpose**: \n",
    "  Multi-Head Attention allows the model to focus on different parts of the sequence from multiple perspectives. Instead of computing a single attention score, the model computes several attention scores in parallel (i.e., **multiple \"heads\"**), each attending to different subspaces of the query, key, and value vectors. This enables the model to capture various relationships between the words in a sequence.\n",
    "\n",
    "- **How it Works**:\n",
    "  - The input consists of query ($Q$), key ($K$), and value ($V$) matrices.\n",
    "  - These matrices are split into multiple \"heads.\" Each head performs its own attention calculation, and the results are concatenated to form a final output.\n",
    "  - By using multiple attention heads, the model can learn different aspects of the data, such as syntactic or semantic relationships between words, allowing it to capture richer, more complex patterns in the input sequence.\n",
    "\n",
    "- **Mathematical Formulation**:\n",
    "  The process for each head in multi-head attention is as follows:\n",
    "  1. Compute attention for each head:  \n",
    "     $$ \\text{Head}_i = \\text{Attention}(Q_i, K_i, V_i) $$\n",
    "  2. Concatenate all heads:  \n",
    "     $$ \\text{MultiHead}(Q, K, V) = [\\text{Head}_1, \\text{Head}_2, \\ldots, \\text{Head}_h] $$\n",
    "  3. Apply a linear transformation (via a learned weight matrix):  \n",
    "     $$ \\text{Output} = W_O \\times \\text{MultiHead}(Q, K, V) $$\n",
    "\n",
    "- **Example**: \n",
    "  - Suppose we have a sentence \"The cat sat on the mat.\"\n",
    "  - The multi-head attention allows the model to attend to different parts of the sentence for each word. For example, one attention head might focus on understanding the relationship between \"cat\" and \"sat,\" while another head might focus on the relationship between \"mat\" and \"the.\"\n",
    "\n",
    "- **Intuition**:\n",
    "  Multi-head attention helps the model process information from multiple angles. If one head focuses on the syntactic relationships between words (such as grammatical structure), another might focus on the semantic meaning (such as identifying the main subject of the sentence). By combining all these perspectives, the model gains a deeper understanding of the sequence.\n",
    "\n",
    "- **Implementation**:\n",
    "* For the **Multi-Head Attention** implementation, we use the keras [`MultiHeadAttention`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) layer. For further details about how to split the query matrix Q, key matrix K, and value matrix V into different heads, we can look through the implementation of `MultiHeadAttention` layer in keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='6-2'></a>\n",
    "\n",
    "## 6.2. **Feed Forward Neural Network (FFN)**\n",
    "\n",
    "After passing through the multi-head attention layer, the output is fed into a **Feed Forward Neural Network (FFN)**. This network is applied independently to each position (token) in the sequence.\n",
    "\n",
    "- **Purpose**: \n",
    "  The FFN transforms the output of the attention layer, adding non-linearity and allowing the model to learn more complex patterns.\n",
    "\n",
    "- **Structure**:\n",
    "  The feed forward neural network typically consists of:\n",
    "  1. **Dense Layer 1**: A fully connected layer with an activation function (often ReLU) to introduce non-linearity.\n",
    "  2. **Dense Layer 2**: Another fully connected layer, usually with a linear activation function, to reduce the dimensionality back to the original embedding size.\n",
    "\n",
    "- **Mathematical Formulation**:\n",
    "  The feed-forward network is applied to each position individually, meaning the same transformation is applied to each token’s representation independently.\n",
    "  $$ \\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1) W_2 + b_2 $$\n",
    "\n",
    "  Here:\n",
    "  - $x$ is the input (the output from the multi-head attention layer).\n",
    "  - $W_1, W_2$ are weight matrices.\n",
    "  - $b_1, b_2$ are bias vectors.\n",
    "  - ReLU is the activation function applied in the first dense layer.\n",
    "\n",
    "- **Example**: \n",
    "  - For a token like \"cat,\" the FFN might map its representation to a new space (using the weights $W_1$) and then apply a non-linear activation function (like ReLU). Then, it will be mapped back to the original space using $W_2$.\n",
    "  \n",
    "- **Intuition**:\n",
    "  The feed forward neural network allows the model to perform more complex transformations. After attending to different parts of the sequence using multi-head attention, the FFN adds the necessary non-linearity and helps the model generalize better.\n",
    "   \n",
    "- **Implementation**: \n",
    "  we will use the [Sequential API](https://keras.io/api/models/sequential/) with two dense layers to built the feed forward neural network layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim, activation='relu', kernel_initializer='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Builds a feed-forward neural network with two Dense layers for the Transformer Encoder.\n",
    "\n",
    "    Parameters:\n",
    "    - embedding_dim (int): Dimensionality of the output embeddings (d_model).\n",
    "    - fully_connected_dim (int): Dimensionality of the hidden layer (dff).\n",
    "    - activation (str): Activation function to use in the hidden layer (default: 'relu').\n",
    "    - kernel_initializer (str): Initializer for kernel weights (default: 'glorot_uniform').\n",
    "    - dropout_rate (float): Dropout rate for regularization (default: 0.0).\n",
    "\n",
    "    Returns:\n",
    "    - tf.keras.Sequential: A feed-forward network with two Dense layers.\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            fully_connected_dim,\n",
    "            activation=activation,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            name='hidden_dense_layer'  # (batch_size, seq_len, dff)\n",
    "        ),\n",
    "        tf.keras.layers.Dense(\n",
    "            embedding_dim,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            name='output_dense_layer'  # (batch_size, seq_len, d_model)\n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='6-3'></a>\n",
    "\n",
    "## 6.3. **Layer Normalization and Residual Connections**\n",
    "\n",
    "Both the multi-head attention and the feed forward network layers are typically wrapped with **layer normalization** and **residual connections**:\n",
    "\n",
    "- **Residual Connection**: The output of each layer is added to its input before passing through the next layer. This helps with training deep models by alleviating the vanishing gradient problem.\n",
    "  \n",
    "  $$ \\text{Output}_\\text{Attention} = \\text{LayerNorm}(x + \\text{MultiHead}(Q, K, V)) $$\n",
    "\n",
    "- **Layer Normalization**: After adding the residual connection, layer normalization is applied to stabilize training by normalizing the output.\n",
    "\n",
    "- **Intuition**: \n",
    "  Residual connections allow gradients to flow more easily during backpropagation, while layer normalization ensures that the output values are on a similar scale, making training more stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6-4'></a>\n",
    "\n",
    "## 6.4. **Encoder Layer**\n",
    "\n",
    "The Encoder Layer is a fundamental component of the Transformer architecture, combining multi-head self-attention with a feed-forward neural network (FFN). Additionally, it leverages residual connections and layer normalization to enhance training stability and efficiency (Figure 2a).\n",
    "\n",
    "### Encoder Layer Implementation\n",
    "\n",
    "We will implement the `EncoderLayer()` (as depicted in Figure 2a) using the `call()` method. This implementation integrates all key Transformer concepts to form a single encoder block, which can be stacked to create the full encoder.\n",
    "\n",
    "#### **Key Functional Steps**:\n",
    "1. **Multi-Head Attention**:\n",
    "   - Passes the input tensor as `Q`, `K`, and `V` (self-attention) along with an optional boolean mask to the multi-head attention layer.\n",
    "   -Computes self-attention for each word in the input sequence, enabling focus on related words.\n",
    "   - The `call()` method for the multi-head attention (`self.mha`) accepts:\n",
    "    - **Query, Key, Value**: Tensors of shape `(batch_size, seq_len, embedding_dim)`.\n",
    "    - **Attention Mask**: Boolean tensor of shape `(batch_size, seq_len, seq_len)`. Specifies which elements should not be attended to (e.g., padding).\n",
    "   - **Dropout** is applied during training to the network to reduce overfitting.\n",
    "2. **Residual Connection and Layer Normalization**\n",
    "   - Residual Connection adds the original input to the output of the multi-head attention layer (skip connection). \n",
    "   - Residual Connection help prevent vanishing gradients and maintain smooth training.\n",
    "   - Layer Normalization normalizes the result to stabilize training.\n",
    "3. **Feed-Forward Neural Network (FFN)**:\n",
    "   - Processes each position independently through two dense layers.\n",
    "   - Dropout regularizes the network during training.\n",
    "4. **Final Residual Connection and Layer Normalization**:\n",
    "   - Adds the normalized output of the attention block to the FFN output (skip connection).\n",
    "   - Normalizes the result to complete the encoder layer.\n",
    "\n",
    "#### Key Methods:\n",
    "1. **Initialization (`__init__`)**:\n",
    "   - Defines all necessary layers for the encoder block, ensuring modularity.\n",
    "   - Multi-head attention, feed-forward network, layer normalization, and dropout are initialized with appropriate configurations.\n",
    "\n",
    "2. **Forward Pass (`call`)**:\n",
    "   - **Step 1: Multi-Head Attention**:\n",
    "     - Computes self-attention over the input sequence.\n",
    "     - Adds residual connection from input (`x`) and normalizes.\n",
    "   - **Step 2: Feed-Forward Network**:\n",
    "     - Processes the normalized output position-wise.\n",
    "     - Adds residual connection from the previous step and normalizes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A single Transformer Encoder Layer.\n",
    "\n",
    "    Combines multi-head self-attention, a feed-forward neural network, residual connections,\n",
    "    and layer normalization.\n",
    "\n",
    "    Attributes:\n",
    "    - embedding_dim (int): Dimensionality of input embeddings.\n",
    "    - num_heads (int): Number of attention heads.\n",
    "    - fully_connected_dim (int): Dimensionality of the FFN's hidden layer.\n",
    "    - dropout_rate (float): Dropout rate for regularization.\n",
    "    - layernorm_eps (float): Small epsilon to avoid division by zero in LayerNormalization.\n",
    "\n",
    "    Methods:\n",
    "    - call(x, training, mask): Performs the forward pass for the encoder layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        # Multi-Head Attention Layer\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim, dropout=dropout_rate, name=\"multi_head_attention\")\n",
    "\n",
    "        # Feed-Forward Neural Network (Fully Connected Layer)\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
    "                                  fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps, name=\"layer_norm_mha\")\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps, name=\"layer_norm_ffn\")\n",
    "\n",
    "        # Dropout for FFN\n",
    "        self.dropout_ffn = Dropout(dropout_rate, name=\"ffn_dropout\")\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer.\n",
    "\n",
    "        Args:\n",
    "        - x (Tensor): Input tensor of shape (batch_size, seq_len, embedding_dim).\n",
    "        - training (bool): Whether the layer is in training mode.\n",
    "        - mask (Tensor): Boolean mask to prevent attention to certain positions.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: Output tensor of shape (batch_size, seq_len, embedding_dim).\n",
    "        \"\"\"\n",
    "        # Step 1: Multi-Head Attention\n",
    "        attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask, training=training)  # Self-attention\n",
    "        out1 = self.layernorm1(x + attn_output, training=training)  # Residual connection + Layer normalization\n",
    "\n",
    "        # Step 2: Feed-Forward Neural Network\n",
    "        ffn_output = self.ffn(out1)  # Position-wise feed-forward network\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training)  # Dropout during training\n",
    "        encoder_layer_out = self.layernorm2(out1 + ffn_output, training=training)  # Residual connection + Layer normalization\n",
    "\n",
    "        return encoder_layer_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "\n",
    "# Set fixed seeds for reproducibility\n",
    "seed_value = 42  # You can change this value as desired\n",
    "\n",
    "# Set the seed for NumPy (for any operations using numpy.random)\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set the seed for TensorFlow (for any operations involving tf.random)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# The test function with dynamic calculation of expected output\n",
    "def EncoderLayer_test(target):\n",
    "    # Define test input and mask\n",
    "    q = np.array([[[1, 0, 1, 1], [0, 1, 1, 1], [1, 0, 0, 1]]]).astype(np.float32)\n",
    "    mask = np.array([[1, 0, 1]]).astype(np.float32)  # Shape (1, seq_len, seq_len)\n",
    "\n",
    "    # Instantiate EncoderLayer\n",
    "    embedding_dim = 4\n",
    "    num_heads = 2\n",
    "    fully_connected_dim = 8\n",
    "    encoder_layer = target(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_heads=num_heads,\n",
    "        fully_connected_dim=fully_connected_dim,\n",
    "        dropout_rate=0.1\n",
    "    )\n",
    "\n",
    "\n",
    "    # Test training mode (dynamic calculation)\n",
    "    encoder_output = encoder_layer(q, training=True, mask=mask) # training=True\n",
    "    assert tf.is_tensor(encoder_output), \"Output must be a tensor.\"\n",
    "    assert encoder_output.shape == q.shape, f\"Expected shape {q.shape}, got {encoder_output.shape}.\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "EncoderLayer_test(EncoderLayer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6-5'></a>\n",
    "\n",
    "## 6.5. Full Encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='6-4'></a>\n",
    "\n",
    "## 6.4. **Final Output**\n",
    "\n",
    "The final output of the Encoder is passed to the **Decoder** (in tasks like machine translation). The Encoder’s output contains enriched representations of the input sequence, which the Decoder will use to generate the final output (e.g., translated text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
